{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"# Cleaning data\",\n",
    "   \"id\": \"bc445b4a7cd8fca4\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": [\n",
    "    \"## Robust data cleaning!\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 1. **Better Type Handling**\\n\",\n",
    "    \"- Uses `Int64` (nullable integer) instead of to preserve NaN values `int`\\n\",\n",
    "    \"- Uses `float64` for decimal precision\\n\",\n",
    "    \"- Properly handles data type conversions\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 2. **Comprehensive Data Cleaning**\\n\",\n",
    "    \"- Handles multiple decimal points\\n\",\n",
    "    \"- Removes currency symbols (€, $, £, etc.)\\n\",\n",
    "    \"- Handles European number formats (1.234.567,89)\\n\",\n",
    "    \"- Removes units (m², sqm, etc.)\\n\",\n",
    "    \"- Handles various null representations\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 3. **Data Validation**\\n\",\n",
    "    \"- Min/max value constraints\\n\",\n",
    "    \"- Negative value handling\\n\",\n",
    "    \"- Outlier detection using IQR method\\n\",\n",
    "    \"- Comprehensive reporting\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 4. **Better Error Handling**\\n\",\n",
    "    \"- Checks if columns exist\\n\",\n",
    "    \"- Reports invalid data conversions\\n\",\n",
    "    \"- Provides detailed statistics\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 5. **Production-Ready Features**\\n\",\n",
    "    \"- Proper documentation\\n\",\n",
    "    \"- Progress indicators\\n\",\n",
    "    \"- Data quality checks\\n\",\n",
    "    \"- Validation summary\\n\",\n",
    "    \"- File size reporting\"\n",
    "   ],\n",
    "   \"id\": \"bfdf8bdfe8a8a5cd\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-11-14T14:18:18.926804Z\",\n",
    "     \"start_time\": \"2025-11-14T14:18:17.736688Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ENHANCED HELPER FUNCTIONS\\n\",\n",
    "    \"def clean_numeric_column(df, column, as_int=False, is_price=False, is_percentage=False,\\n\",\n",
    "    \"                         allow_negative=False, min_value=None, max_value=None):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Clean and standardize numeric columns with comprehensive data validation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Parameters:\\n\",\n",
    "    \"    -----------\\n\",\n",
    "    \"    df : pandas.DataFrame\\n\",\n",
    "    \"        The dataframe containing the column to clean\\n\",\n",
    "    \"    column : str\\n\",\n",
    "    \"        Name of the column to clean\\n\",\n",
    "    \"    as_int : bool\\n\",\n",
    "    \"        Convert to integer type (default: False, keeps as float)\\n\",\n",
    "    \"    is_price : bool\\n\",\n",
    "    \"        Special handling for price/currency formats (default: False)\\n\",\n",
    "    \"    is_percentage : bool\\n\",\n",
    "    \"        Special handling for percentage values (default: False)\\n\",\n",
    "    \"    allow_negative : bool\\n\",\n",
    "    \"        Whether to allow negative values (default: False)\\n\",\n",
    "    \"    min_value : float or None\\n\",\n",
    "    \"        Minimum acceptable value, values below are set to NaN\\n\",\n",
    "    \"    max_value : float or None\\n\",\n",
    "    \"        Maximum acceptable value, values above are set to NaN\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"    --------\\n\",\n",
    "    \"    pandas.DataFrame\\n\",\n",
    "    \"        DataFrame with cleaned numeric column\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if column not in df.columns:\\n\",\n",
    "    \"        print(f\\\"Warning: Column '{column}' not found in dataframe\\\")\\n\",\n",
    "    \"        return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Store original non-null count for reporting\\n\",\n",
    "    \"    original_non_null = df[column].notna().sum()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Create a copy to avoid SettingWithCopyWarning\\n\",\n",
    "    \"    cleaned_series = df[column].copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert to string for cleaning, preserve NaN\\n\",\n",
    "    \"    cleaned_series = cleaned_series.astype(str)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Replace common null representations\\n\",\n",
    "    \"    null_patterns = ['nan', 'none', 'null', 'n/a', 'na', '#n/a', '#value!', '#ref!',\\n\",\n",
    "    \"                     '<na>', 'missing', 'MISSING', '--', '']\\n\",\n",
    "    \"    cleaned_series = cleaned_series.replace(null_patterns, np.nan, regex=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # deal with multiple empty spaces\\n\",\n",
    "    \"    cleaned_series = cleaned_series.str.strip()\\n\",\n",
    "    \"    cleaned_series.replace('', np.nan, inplace=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Skip if all values are NaN\\n\",\n",
    "    \"    if cleaned_series.isna().all():\\n\",\n",
    "    \"        df[column] = np.nan\\n\",\n",
    "    \"        print(f\\\"  Warning: Column '{column}' contains only null values\\\")\\n\",\n",
    "    \"        return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Handle percentage values\\n\",\n",
    "    \"    if is_percentage:\\n\",\n",
    "    \"        # Convert percentages like \\\"85%\\\", \\\"85 %\\\", \\\"0.85\\\" to decimal\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'\\\\s*%\\\\s*', '', regex=True)\\n\",\n",
    "    \"        # Check if values are in percentage format (>1) or decimal format (<=1)\\n\",\n",
    "    \"        temp_numeric = pd.to_numeric(cleaned_series, errors='coerce')\\n\",\n",
    "    \"        # If most non-null values are > 1, assume they're percentages\\n\",\n",
    "    \"        if (temp_numeric > 1).sum() > (temp_numeric <= 1).sum():\\n\",\n",
    "    \"            temp_numeric = temp_numeric / 100\\n\",\n",
    "    \"        cleaned_series = temp_numeric\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Handle price/currency values\\n\",\n",
    "    \"    elif is_price:\\n\",\n",
    "    \"        # Remove currency symbols: €, $, £, ¥, CHF, USD, EUR, etc.\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'[€$£¥₹₽¢]', '', regex=True)\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'\\\\b(USD|EUR|GBP|CHF|CAD|AUD|JPY)\\\\b', '',\\n\",\n",
    "    \"                                                   regex=True, flags=re.IGNORECASE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Handle European number format: 1.234.567,89 -> 1234567.89\\n\",\n",
    "    \"        # First, remove thousand separators (dots, spaces, apostrophes)\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'(?<=\\\\d)[.\\\\s\\\\'\\\\u202f\\\\u00a0]+(?=\\\\d{3})', '', regex=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Convert decimal comma to dot: 123,45 -> 123.45\\n\",\n",
    "    \"        # But only if it's the last comma (to avoid issues with thousand separators)\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r',(\\\\d{1,2})$', r'.\\\\1', regex=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Remove any remaining non-numeric characters except decimal point and minus\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'[^\\\\d.\\\\-]', '', regex=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Handle regular numeric values (areas, counts, etc.)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Remove units like m², m2, sqm, sq.m, cm, km, etc.\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'\\\\s*(m[²2]|sqm?|sq\\\\.?\\\\s*m|cm|km|ft²?|square\\\\s*meters?)\\\\s*',\\n\",\n",
    "    \"                                                   '', regex=True, flags=re.IGNORECASE)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Remove thousand separators and normalize decimal separators\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'(?<=\\\\d)[.\\\\s\\\\'\\\\u202f\\\\u00a0]+(?=\\\\d{3})', '', regex=True)\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(',', '.', regex=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Remove any remaining non-numeric characters except decimal point and minus\\n\",\n",
    "    \"        cleaned_series = cleaned_series.str.replace(r'[^\\\\d.\\\\-]', '', regex=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Handle multiple decimal points (keep only the last one)\\n\",\n",
    "    \"    def fix_multiple_decimals(val):\\n\",\n",
    "    \"        if pd.isna(val) or val == '':\\n\",\n",
    "    \"            return np.nan\\n\",\n",
    "    \"        if isinstance(val, str) and val.count('.') > 1:\\n\",\n",
    "    \"            parts = val.rsplit('.', 1)  # Split at the last decimal point\\n\",\n",
    "    \"            return parts[0].replace('.', '') + '.' + parts[1]\\n\",\n",
    "    \"        return val\\n\",\n",
    "    \"\\n\",\n",
    "    \"    cleaned_series = cleaned_series.apply(fix_multiple_decimals)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert to numeric, coercing errors to NaN\\n\",\n",
    "    \"    cleaned_series = pd.to_numeric(cleaned_series, errors='coerce')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Handle negative values\\n\",\n",
    "    \"    if not allow_negative:\\n\",\n",
    "    \"        negative_count = (cleaned_series < 0).sum()\\n\",\n",
    "    \"        if negative_count > 0:\\n\",\n",
    "    \"            print(f\\\"  Warning: Found {negative_count} negative values in '{column}', setting to NaN\\\")\\n\",\n",
    "    \"        cleaned_series = cleaned_series.where(cleaned_series >= 0, np.nan)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Apply min/max value constraints\\n\",\n",
    "    \"    if min_value is not None:\\n\",\n",
    "    \"        out_of_range = (cleaned_series < min_value).sum()\\n\",\n",
    "    \"        if out_of_range > 0:\\n\",\n",
    "    \"            print(f\\\"  Warning: Found {out_of_range} values below minimum ({min_value}) in '{column}'\\\")\\n\",\n",
    "    \"        cleaned_series = cleaned_series.where(cleaned_series >= min_value, np.nan)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if max_value is not None:\\n\",\n",
    "    \"        out_of_range = (cleaned_series > max_value).sum()\\n\",\n",
    "    \"        if out_of_range > 0:\\n\",\n",
    "    \"            print(f\\\"  Warning: Found {out_of_range} values above maximum ({max_value}) in '{column}'\\\")\\n\",\n",
    "    \"        cleaned_series = cleaned_series.where(cleaned_series <= max_value, np.nan)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Convert to appropriate dtype\\n\",\n",
    "    \"    if as_int:\\n\",\n",
    "    \"        # Use Int64 (nullable integer) instead of int to preserve NaN\\n\",\n",
    "    \"        cleaned_series = cleaned_series.astype('Int64')\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Keep as float64 for decimal precision\\n\",\n",
    "    \"        cleaned_series = cleaned_series.astype('float64')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Report cleaning results\\n\",\n",
    "    \"    final_non_null = cleaned_series.notna().sum()\\n\",\n",
    "    \"    nulls_created = original_non_null - final_non_null\\n\",\n",
    "    \"    if nulls_created > 0:\\n\",\n",
    "    \"        print(f\\\"  Cleaned '{column}': {nulls_created} values converted to null due to invalid data\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Assign back to dataframe\\n\",\n",
    "    \"    df[column] = cleaned_series\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return df\"\n",
    "   ],\n",
    "   \"id\": \"c56bc88ee187da14\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 1\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"Validate with IQR and Z-score\",\n",
    "   \"id\": \"c8c0a8fb8ee60881\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-11-14T14:18:25.464905Z\",\n",
    "     \"start_time\": \"2025-11-14T14:18:18.970195Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from scipy.stats import zscore # We'll use scipy for an easy Z-Score calculation\\n\",\n",
    "    \"\\n\",\n",
    "    \"def validate_and_report(df, stats=True):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Validate and report statistics for a cleaned numeric column, including\\n\",\n",
    "    \"    outlier detection using IQR and Z-Score methods.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Parameters:\\n\",\n",
    "    \"    -----------\\n\",\n",
    "    \"    df : pandas.DataFrame\\n\",\n",
    "    \"        The dataframe containing the column\\n\",\n",
    "    \"    stats : bool\\n\",\n",
    "    \"        Whether to print detailed statistics\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    for column in df.columns:\\n\",\n",
    "    \"        col_data = df[column]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        print(f\\\"\\\\n--- Statistics for '{column}' ---\\\")\\n\",\n",
    "    \"        print(f\\\"  Total rows: {len(col_data)}\\\")\\n\",\n",
    "    \"        print(f\\\"  Non-null values: {col_data.notna().sum()} ({col_data.notna().sum()/len(col_data)*100:.1f}%)\\\")\\n\",\n",
    "    \"        print(f\\\"  Null values: {col_data.isna().sum()} ({col_data.isna().sum()/len(col_data)*100:.1f}%)\\\")\\n\",\n",
    "    \"        print(f\\\"  Data type: {col_data.dtype}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Check if the column is numeric and has non-null data to calculate statistics\\n\",\n",
    "    \"        if stats and col_data.notna().any() and (col_data.dtype.kind in 'fi'):\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # Use only non-null values for statistical calculations\\n\",\n",
    "    \"            non_null_data = col_data.dropna()\\n\",\n",
    "    \"\\n\",\n",
    "    \"            print(f\\\"  Min: {non_null_data.min():.2f}\\\")\\n\",\n",
    "    \"            print(f\\\"  Max: {non_null_data.max():.2f}\\\")\\n\",\n",
    "    \"            print(f\\\"  Mean: {non_null_data.mean():.2f}\\\")\\n\",\n",
    "    \"            print(f\\\"  Median: {non_null_data.median():.2f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # --- 1. IQR Method ---\\n\",\n",
    "    \"            Q1 = non_null_data.quantile(0.25)\\n\",\n",
    "    \"            Q3 = non_null_data.quantile(0.75)\\n\",\n",
    "    \"            IQR = Q3 - Q1\\n\",\n",
    "    \"            outliers_iqr = ((non_null_data < (Q1 - 1.5 * IQR)) | (non_null_data > (Q3 + 1.5 * IQR))).sum()\\n\",\n",
    "    \"            if outliers_iqr > 0:\\n\",\n",
    "    \"                print(f\\\"  ⚠️  Potential outliers (IQR method): {outliers_iqr} ({outliers_iqr/len(non_null_data)*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # --- 2. Z-Score Method ---\\n\",\n",
    "    \"            # Calculate Z-score for non-null data\\n\",\n",
    "    \"            z_scores = zscore(non_null_data)\\n\",\n",
    "    \"            # Outliers are typically defined as those with |Z-score| > 3\\n\",\n",
    "    \"            outliers_zscore = (np.abs(z_scores) > 3).sum()\\n\",\n",
    "    \"            if outliers_zscore > 0:\\n\",\n",
    "    \"                print(f\\\"  ⚠️  Potential outliers (Z-Score > 3): {outliers_zscore} ({outliers_zscore/len(non_null_data)*100:.1f}%)\\\")\"\n",
    "   ],\n",
    "   \"id\": \"eda538ad0ce5c343\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 2\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"Reporting with LOF\",\n",
    "   \"id\": \"1b96ebde5f5bc845\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-11-14T14:18:26.237754Z\",\n",
    "     \"start_time\": \"2025-11-14T14:18:25.646215Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from sklearn.neighbors import LocalOutlierFactor\\n\",\n",
    "    \"\\n\",\n",
    "    \"def report_lof_outliers(df, features, contamination='auto', n_neighbors=20):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Reports the number of Local Outlier Factor (LOF) anomalies\\n\",\n",
    "    \"    on selected numerical features.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Parameters:\\n\",\n",
    "    \"    -----------\\n\",\n",
    "    \"    df : pandas.DataFrame\\n\",\n",
    "    \"        The full dataset.\\n\",\n",
    "    \"    features : list\\n\",\n",
    "    \"        List of numeric column names to use for LOF calculation\\n\",\n",
    "    \"        (e.g., ['price', 'total_area_sqm', 'nbr_bedrooms']).\\n\",\n",
    "    \"    contamination : float or 'auto'\\n\",\n",
    "    \"        The expected proportion of outliers in the dataset.\\n\",\n",
    "    \"    n_neighbors : int\\n\",\n",
    "    \"        Number of neighbors to consider for local density.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\n--- Multivariate LOF Outlier Detection ---\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 1. Prepare data (drop N/A and select features)\\n\",\n",
    "    \"    df_lof = df[features].dropna().drop_duplicates()\\n\",\n",
    "    \"    print(f\\\"  Analysing {len(df_lof)} rows with features: {features}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 2. Initialize and Fit LOF Model\\n\",\n",
    "    \"    # `novelty=False` is used for outlier detection (vs. novelty detection)\\n\",\n",
    "    \"    lof = LocalOutlierFactor(n_neighbors=n_neighbors,\\n\",\n",
    "    \"                             contamination=contamination,\\n\",\n",
    "    \"                             novelty=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Predict returns -1 for outliers and 1 for inliers\\n\",\n",
    "    \"    outlier_labels = lof.fit_predict(df_lof)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 3. Calculate Results\\n\",\n",
    "    \"    # Count of LOF outliers (where the prediction is -1)\\n\",\n",
    "    \"    outliers_lof = (outlier_labels == -1).sum()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 4. Report\\n\",\n",
    "    \"    if outliers_lof > 0:\\n\",\n",
    "    \"        print(f\\\"  ⚠️  Multivariate LOF Outliers: {outliers_lof} ({outliers_lof/len(df_lof)*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Optionally, get the outlier scores\\n\",\n",
    "    \"        # lof_scores = lof.negative_outlier_factor_\\n\",\n",
    "    \"        # print(\\\"  Top 5 LOF scores (most anomalous):\\\", np.sort(lof_scores)[:5])\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"  ✅ No multivariate LOF outliers detected.\\\")\"\n",
    "   ],\n",
    "   \"id\": \"db85751d90c1adf5\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 3\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"This class should contain all the functions that helpls us clean the data.\",\n",
    "   \"id\": \"a96040a2d145476b\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true,\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-11-14T14:18:26.278760Z\",\n",
    "     \"start_time\": \"2025-11-14T14:18:26.250446Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"# MAIN CLASS\\n\",\n",
    "    \"class DataProcessing:\\n\",\n",
    "    \"    def __init__(self, file_path='../Kristin/sample_data_copy/properties.csv'):\\n\",\n",
    "    \"        # update line of code above with local CSV file path to load data <---\\n\",\n",
    "    \"        # ensure old df is cleared so a new file will truly be read (and not a cached file)\\n\",\n",
    "    \"        if hasattr(self, 'df'):\\n\",\n",
    "    \"            del self.df\\n\",\n",
    "    \"        # auto-detect separator in CSV file\\n\",\n",
    "    \"        with open(file_path, 'r', encoding='utf-8') as f:\\n\",\n",
    "    \"            first_line = f.readline()\\n\",\n",
    "    \"            sep = ';' if ';' in first_line else ','  # choose ';' if present, else ','\\n\",\n",
    "    \"        # load full csv file\\n\",\n",
    "    \"        self.df = pd.read_csv(file_path, sep=sep, dtype={\\\"id\\\": str}, low_memory=False)\\n\",\n",
    "    \"        print(\\\"Detected separator:\\\", repr(sep))\\n\",\n",
    "    \"        print(\\\"\\\\nBefore any cleaning:\\\")\\n\",\n",
    "    \"        print(self.df.dtypes,\\\"\\\\n\\\")\\n\",\n",
    "    \"        print(self.df.head(5))\\n\",\n",
    "    \"        print(\\\"\\\\nNumber of rows raw data loaded:\\\", len(self.df))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def process_data(self): # main method to process data, further methods detailed below\\n\",\n",
    "    \"        self.clean_price()\\n\",\n",
    "    \"        self.clean_areas()\\n\",\n",
    "    \"        self.convert_yes_no_columns()\\n\",\n",
    "    \"        self.clean_other_numeric_columns()\\n\",\n",
    "    \"        self.remove_duplicates()\\n\",\n",
    "    \"        self.remove_empty_rows()\\n\",\n",
    "    \"        self.clean_missing()\\n\",\n",
    "    \"        self.strip_text_columns()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def clean_price(self): # method to clean the price column\\n\",\n",
    "    \"        if 'price' in self.df.columns:\\n\",\n",
    "    \"            self.df = clean_numeric_column(self.df, 'price', as_int=True, is_price=True)\\n\",\n",
    "    \"            print(\\\"Cleaning price fields...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def clean_areas(self): # method to clean the area columns\\n\",\n",
    "    \"        for col in ['total_area_sqm', 'terrace_sqm', 'garden_sqm']:\\n\",\n",
    "    \"            if col in self.df.columns:\\n\",\n",
    "    \"                # Remove units like 'm2', 'm²' (case-insensitive)\\n\",\n",
    "    \"                self.df[col] = self.df[col].astype(str).str.replace(r'\\\\s*m[²2]', '', regex=True)\\n\",\n",
    "    \"                self.df = clean_numeric_column(self.df, col, as_int=True)\\n\",\n",
    "    \"            print(\\\"Cleaning area fields...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def convert_yes_no_columns(self): # method to convert yes/no to 1/0\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        Converts 'yes'/'no' style columns to 1, 0, or pd.NA (for missing data).\\n\",\n",
    "    \"        The key is using .replace() to handle mapping while preserving original NaNs.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        # Mapping: 'yes'/'y' to 1, 'no'/'n' to 0\\n\",\n",
    "    \"        yes_no_map = {'yes': 1, 'y': 1, 'no': 0, 'n': 0}\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # The columns of interest (assuming these are the correct column names)\\n\",\n",
    "    \"        for col in ['fl_furnished', 'fl_open_fire', 'fl_swimming_pool']:\\n\",\n",
    "    \"            if col in self.df.columns:\\n\",\n",
    "    \"                # 1. Strip and lowercase all non-NaN values\\n\",\n",
    "    \"                # We use .str.lower() on a Series that is already stripped/lowercased.\\n\",\n",
    "    \"                # Crucially, we use fillna('') to temporarily convert NaNs to empty string\\n\",\n",
    "    \"                # so the .str methods can be applied without error, then convert back.\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Use .apply() with a lambda to handle string cleaning while preserving NaN\\n\",\n",
    "    \"                cleaned_series = self.df[col].apply(lambda x: str(x).strip().lower()\\n\",\n",
    "    \"                                                    if pd.notna(x) and x is not None else np.nan)\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # 2. Use replace() on the cleaned series, which preserves existing NaN values\\n\",\n",
    "    \"                # Use pd.NA as the final placeholder for unmapped/missing data\\n\",\n",
    "    \"                self.df[col] = (\\n\",\n",
    "    \"                    cleaned_series\\n\",\n",
    "    \"                    .replace(yes_no_map)\\n\",\n",
    "    \"                    .replace({np.nan: pd.NA}) # Ensure explicit NaN values are pd.NA\\n\",\n",
    "    \"                    .astype('Int64') # Use nullable integer type to allow pd.NA\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"        print(\\\"Converting Yes/No columns to 1, 0, or Nan integers...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def clean_other_numeric_columns(self): # convert other numeric columns to integers\\n\",\n",
    "    \"        for col in ['nbr_bedrooms', 'nbr_frontages', 'construction_year']:\\n\",\n",
    "    \"            if col in self.df.columns:\\n\",\n",
    "    \"                self.df = clean_numeric_column(self.df, col, as_int=True)\\n\",\n",
    "    \"        print(\\\"Cleaning other numeric fields...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def remove_duplicates(self): # method to remove duplicates based on all columns except id\\n\",\n",
    "    \"        cols_to_check = [col for col in self.df.columns if col != 'id']\\n\",\n",
    "    \"        # Find duplicates\\n\",\n",
    "    \"        duplicates_mask = self.df.duplicated(subset=cols_to_check, keep=False)\\n\",\n",
    "    \"        num_duplicates = duplicates_mask.sum()\\n\",\n",
    "    \"        if num_duplicates > 0:\\n\",\n",
    "    \"            print(f\\\"\\\\nFound {num_duplicates} duplicate row(s)\\\")\\n\",\n",
    "    \"            # print(self.df[duplicates_mask].sort_values(by=cols_to_check).head(10)) # showing first 10 duplicates\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(\\\"\\\\nNo duplicate rows found.\\\")\\n\",\n",
    "    \"        self.df.drop_duplicates(subset=cols_to_check, keep='first', inplace=True)\\n\",\n",
    "    \"        print(\\\"Removing duplicates...\\\")\\n\",\n",
    "    \"        print(f\\\"Number of rows left after removing duplicates = {len(self.df)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def remove_empty_rows(self): # method to remove rows where id is missing or all other fields are empty\\n\",\n",
    "    \"        critical_cols = [col for col in self.df.columns if col != 'id']\\n\",\n",
    "    \"        # identify rows where all non-id columns are empty\\n\",\n",
    "    \"        # for numeric columns check NaN, for others, check empty string after stripping\\n\",\n",
    "    \"        empty_mask = pd.Series(True, index=self.df.index)\\n\",\n",
    "    \"        for col in critical_cols:\\n\",\n",
    "    \"            if self.df[col].dtype in [int, float]:\\n\",\n",
    "    \"                col_empty = self.df[col].isna()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                col_empty = self.df[col].astype(str).str.strip().eq('') | self.df[col].isna()\\n\",\n",
    "    \"            empty_mask &= col_empty\\n\",\n",
    "    \"\\n\",\n",
    "    \"        missing_id_mask = self.df['id'].isna() | (self.df['id'].astype(str).str.strip() == '') # remove rows without id\\n\",\n",
    "    \"        num_missing = missing_id_mask.sum()\\n\",\n",
    "    \"        print(f\\\"\\\\nFound {num_missing} row(s) with missing id\\\")\\n\",\n",
    "    \"        self.df = self.df.loc[~missing_id_mask] # drop rows with missing id\\n\",\n",
    "    \"\\n\",\n",
    "    \"        rows_to_drop = self.df[empty_mask].index # remove rows where all non-id fields are empty\\n\",\n",
    "    \"        num_empty_rows = len(rows_to_drop)\\n\",\n",
    "    \"        print(f\\\"Found {num_empty_rows} row(s) where all non-id fields are empty\\\")\\n\",\n",
    "    \"        if num_empty_rows >0:\\n\",\n",
    "    \"            print(\\\"Preview of up to 10 rows to be removed (by id):\\\")\\n\",\n",
    "    \"            display(self.df.loc[rows_to_drop[:10], :])  # this will print the first 10 rows to be removed\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.df.drop(index=rows_to_drop, inplace=True)\\n\",\n",
    "    \"        print(\\\"Removing empty rows...\\\")\\n\",\n",
    "    \"        print(f\\\"Number of rows left after removing empty rows = {len(self.df)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def strip_text_columns(self): # strip leading and trailing spaces from text\\n\",\n",
    "    \"        text_cols = self.df.select_dtypes(include='object').columns\\n\",\n",
    "    \"        for col in text_cols:\\n\",\n",
    "    \"            self.df[col] = self.df[col].astype(str).str.strip()\\n\",\n",
    "    \"        print(\\\"\\\\nStripping leading/trailing spaces from all text columns...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def clean_missing(self): # method to clean any \\\"MISSING\\\" string with NaN and ensure missing fields remain NaN\\n\",\n",
    "    \"        for col in self.df.columns:\\n\",\n",
    "    \"            if col != 'id':\\n\",\n",
    "    \"                # Convert 'MISSING' (case-insensitive) to NaN\\n\",\n",
    "    \"                self.df[col] = self.df[col].replace(r'(?i)^MISSING$', np.nan, regex=True)\\n\",\n",
    "    \"                # Also ensure empty strings are treated as NaN\\n\",\n",
    "    \"                if self.df[col].dtype == 'object':\\n\",\n",
    "    \"                    self.df[col] = self.df[col].replace(r'^\\\\s*$', np.nan, regex=True)\\n\",\n",
    "    \"        print(\\\"\\\\nCleaning missing values: 'MISSING' and converting empty strings to NaN...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def save_to_csv(self, output_path='../Kristin/cleaned_properties.csv'): # method to create the output file, update file path <---\\n\",\n",
    "    \"        self.df.to_csv(output_path, index=False)\\n\",\n",
    "    \"        print(\\\"\\\\nSaving cleaned output as csv ...\\\")\"\n",
    "   ],\n",
    "   \"id\": \"8e8b43cfbbdd8f79\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 4\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"log-transforming function for one column\",\n",
    "   \"id\": \"c7f75fded77e8635\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-11-14T14:18:26.306294Z\",\n",
    "     \"start_time\": \"2025-11-14T14:18:26.292173Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"def log_transform_column(df: pd.DataFrame, column_name: str, new_col_name: str = None) -> pd.DataFrame:\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Applies the natural log transformation (log(1+x)) to a specified column\\n\",\n",
    "    \"    and adds it as a new column to the DataFrame.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Log transformation is primarily used for features with a high positive skew\\n\",\n",
    "    \"    (like price or area) to make the distribution more normal.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Parameters:\\n\",\n",
    "    \"    -----------\\n\",\n",
    "    \"    df : pd.DataFrame\\n\",\n",
    "    \"        The input DataFrame.\\n\",\n",
    "    \"    column_name : str\\n\",\n",
    "    \"        The name of the column to be transformed (e.g., 'price').\\n\",\n",
    "    \"    new_col_name : str, optional\\n\",\n",
    "    \"        The name of the new log-transformed column.\\n\",\n",
    "    \"        Defaults to f'log_{column_name}'.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"    --------\\n\",\n",
    "    \"    pd.DataFrame\\n\",\n",
    "    \"        The DataFrame with the new log-transformed column added.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    if new_col_name is None:\\n\",\n",
    "    \"        new_col_name = f'log_{column_name}'\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 1. Check if the column exists\\n\",\n",
    "    \"    if column_name not in df.columns:\\n\",\n",
    "    \"        print(f\\\"Error: Column '{column_name}' not found in DataFrame.\\\")\\n\",\n",
    "    \"        return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 2. Check for non-positive values (logarithm is undefined for <= 0)\\n\",\n",
    "    \"    # Since we use log1p, we only check for negative values\\n\",\n",
    "    \"    if (df[column_name] < 0).any():\\n\",\n",
    "    \"        print(f\\\"Warning: Column '{column_name}' contains negative values. \\\"\\n\",\n",
    "    \"              \\\"Applying log transformation to negative numbers is problematic.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # 3. Apply the log(1 + x) transformation (log1p)\\n\",\n",
    "    \"        # We use .copy() to ensure we are operating on a new DataFrame\\n\",\n",
    "    \"        df_copy = df.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # We use .fillna(0) inside log1p to handle any NaN values gracefully\\n\",\n",
    "    \"        # (they become log(1+0) = 0 in the new column, or NaN if they should remain)\\n\",\n",
    "    \"        # If NaN should remain NaN after transformation, use .dropna() first or ensure NaNs are skipped.\\n\",\n",
    "    \"        # Here we apply it directly, NaNs will result in NaNs in the new column.\\n\",\n",
    "    \"        df_copy[new_col_name] = np.log1p(df_copy[column_name])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        print(f\\\"✅ Successfully created new column '{new_col_name}' using np.log1p.\\\")\\n\",\n",
    "    \"        return df_copy\\n\",\n",
    "    \"\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"An error occurred during log transformation: {e}\\\")\\n\",\n",
    "    \"        return df\"\n",
    "   ],\n",
    "   \"id\": \"7b56406a3a615f7d\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 5\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-11-14T14:18:39.146070Z\",\n",
    "     \"start_time\": \"2025-11-14T14:18:26.318719Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"dp = DataProcessing(file_path='./sample_data_copy/properties.csv')  # adjust path\\n\",\n",
    "    \"dp.process_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"validate_and_report(dp.df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"log_transformed_price = log_transform_column(dp.df, 'price')\\n\",\n",
    "    \"report_lof_outliers(log_transformed_price, ['log_price', 'total_area_sqm', 'garden_sqm'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"dp.save_to_csv('./cleaned_properties.csv')\"\n",
    "   ],\n",
    "   \"id\": \"93a88b4f94d17b5a\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Detected separator: ','\\n\",\n",
    "      \"\\n\",\n",
    "      \"Before any cleaning:\\n\",\n",
    "      \"id                                 object\\n\",\n",
    "      \"price                             float64\\n\",\n",
    "      \"property_type                      object\\n\",\n",
    "      \"subproperty_type                   object\\n\",\n",
    "      \"region                             object\\n\",\n",
    "      \"province                           object\\n\",\n",
    "      \"locality                           object\\n\",\n",
    "      \"zip_code                            int64\\n\",\n",
    "      \"latitude                          float64\\n\",\n",
    "      \"longitude                         float64\\n\",\n",
    "      \"construction_year                 float64\\n\",\n",
    "      \"total_area_sqm                    float64\\n\",\n",
    "      \"surface_land_sqm                  float64\\n\",\n",
    "      \"nbr_frontages                     float64\\n\",\n",
    "      \"nbr_bedrooms                      float64\\n\",\n",
    "      \"equipped_kitchen                   object\\n\",\n",
    "      \"fl_furnished                        int64\\n\",\n",
    "      \"fl_open_fire                        int64\\n\",\n",
    "      \"fl_terrace                          int64\\n\",\n",
    "      \"terrace_sqm                       float64\\n\",\n",
    "      \"fl_garden                           int64\\n\",\n",
    "      \"garden_sqm                        float64\\n\",\n",
    "      \"fl_swimming_pool                    int64\\n\",\n",
    "      \"fl_floodzone                        int64\\n\",\n",
    "      \"state_building                     object\\n\",\n",
    "      \"primary_energy_consumption_sqm    float64\\n\",\n",
    "      \"epc                                object\\n\",\n",
    "      \"heating_type                       object\\n\",\n",
    "      \"fl_double_glazing                   int64\\n\",\n",
    "      \"cadastral_income                  float64\\n\",\n",
    "      \"dtype: object \\n\",\n",
    "      \"\\n\",\n",
    "      \"         id     price property_type subproperty_type            region  \\\\\\n\",\n",
    "      \"0  34221000  225000.0     APARTMENT        APARTMENT          Flanders   \\n\",\n",
    "      \"1   2104000  449000.0         HOUSE            HOUSE          Flanders   \\n\",\n",
    "      \"2  34036000  335000.0     APARTMENT        APARTMENT  Brussels-Capital   \\n\",\n",
    "      \"3  58496000  501000.0         HOUSE            HOUSE          Flanders   \\n\",\n",
    "      \"4  48727000  982700.0     APARTMENT           DUPLEX          Wallonia   \\n\",\n",
    "      \"\\n\",\n",
    "      \"          province  locality  zip_code   latitude  longitude  ...  fl_garden  \\\\\\n\",\n",
    "      \"0          Antwerp   Antwerp      2050  51.217172   4.379982  ...          0   \\n\",\n",
    "      \"1    East Flanders      Gent      9185  51.174944   3.845248  ...          0   \\n\",\n",
    "      \"2         Brussels  Brussels      1070  50.842043   4.334543  ...          0   \\n\",\n",
    "      \"3          Antwerp  Turnhout      2275  51.238312   4.817192  ...          0   \\n\",\n",
    "      \"4  Walloon Brabant  Nivelles      1410        NaN        NaN  ...          1   \\n\",\n",
    "      \"\\n\",\n",
    "      \"   garden_sqm  fl_swimming_pool  fl_floodzone  state_building  \\\\\\n\",\n",
    "      \"0         0.0                 0             0         MISSING   \\n\",\n",
    "      \"1         0.0                 0             0         MISSING   \\n\",\n",
    "      \"2         0.0                 0             1          AS_NEW   \\n\",\n",
    "      \"3         0.0                 0             1         MISSING   \\n\",\n",
    "      \"4       142.0                 0             0          AS_NEW   \\n\",\n",
    "      \"\\n\",\n",
    "      \"  primary_energy_consumption_sqm      epc  heating_type  fl_double_glazing  \\\\\\n\",\n",
    "      \"0                          231.0        C           GAS                  1   \\n\",\n",
    "      \"1                          221.0        C       MISSING                  1   \\n\",\n",
    "      \"2                            NaN  MISSING           GAS                  0   \\n\",\n",
    "      \"3                           99.0        A       MISSING                  0   \\n\",\n",
    "      \"4                           19.0       A+           GAS                  0   \\n\",\n",
    "      \"\\n\",\n",
    "      \"   cadastral_income  \\n\",\n",
    "      \"0             922.0  \\n\",\n",
    "      \"1             406.0  \\n\",\n",
    "      \"2               NaN  \\n\",\n",
    "      \"3               NaN  \\n\",\n",
    "      \"4               NaN  \\n\",\n",
    "      \"\\n\",\n",
    "      \"[5 rows x 30 columns]\\n\",\n",
    "      \"\\n\",\n",
    "      \"Number of rows raw data loaded: 75511\\n\",\n",
    "      \"Cleaning price fields...\\n\",\n",
    "      \"  Cleaned 'total_area_sqm': 7615 values converted to null due to invalid data\\n\",\n",
    "      \"Cleaning area fields...\\n\",\n",
    "      \"  Cleaned 'terrace_sqm': 13140 values converted to null due to invalid data\\n\",\n",
    "      \"Cleaning area fields...\\n\",\n",
    "      \"  Cleaned 'garden_sqm': 2939 values converted to null due to invalid data\\n\",\n",
    "      \"Cleaning area fields...\\n\",\n",
    "      \"Converting Yes/No columns to 1, 0, or Nan integers...\\n\",\n",
    "      \"Cleaning other numeric fields...\\n\",\n",
    "      \"\\n\",\n",
    "      \"Found 6 duplicate row(s)\\n\",\n",
    "      \"Removing duplicates...\\n\",\n",
    "      \"Number of rows left after removing duplicates = 75508\\n\",\n",
    "      \"\\n\",\n",
    "      \"Found 0 row(s) with missing id\\n\",\n",
    "      \"Found 0 row(s) where all non-id fields are empty\\n\",\n",
    "      \"Removing empty rows...\\n\",\n",
    "      \"Number of rows left after removing empty rows = 75508\\n\",\n",
    "      \"\\n\",\n",
    "      \"Cleaning missing values: 'MISSING' and converting empty strings to NaN...\\n\",\n",
    "      \"\\n\",\n",
    "      \"Stripping leading/trailing spaces from all text columns...\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'id' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'price' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 76000.00\\n\",\n",
    "      \"  Max: 22500000.00\\n\",\n",
    "      \"  Mean: 422772.96\\n\",\n",
    "      \"  Median: 329000.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 5800 (7.7%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 1253 (1.7%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'property_type' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'subproperty_type' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'region' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'province' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'locality' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'zip_code' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: int64\\n\",\n",
    "      \"  Min: 1000.00\\n\",\n",
    "      \"  Max: 9992.00\\n\",\n",
    "      \"  Mean: 5144.67\\n\",\n",
    "      \"  Median: 4683.00\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'latitude' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 61410 (81.3%)\\n\",\n",
    "      \"  Null values: 14098 (18.7%)\\n\",\n",
    "      \"  Data type: float64\\n\",\n",
    "      \"  Min: 25.76\\n\",\n",
    "      \"  Max: 52.43\\n\",\n",
    "      \"  Mean: 50.89\\n\",\n",
    "      \"  Median: 50.90\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 1302 (2.1%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 799 (1.3%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'longitude' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 61410 (81.3%)\\n\",\n",
    "      \"  Null values: 14098 (18.7%)\\n\",\n",
    "      \"  Data type: float64\\n\",\n",
    "      \"  Min: -80.19\\n\",\n",
    "      \"  Max: 6.39\\n\",\n",
    "      \"  Mean: 4.33\\n\",\n",
    "      \"  Median: 4.38\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 2 (0.0%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 2 (0.0%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'construction_year' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 42120 (55.8%)\\n\",\n",
    "      \"  Null values: 33388 (44.2%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 1753.00\\n\",\n",
    "      \"  Max: 2024.00\\n\",\n",
    "      \"  Mean: 1984.41\\n\",\n",
    "      \"  Median: 1994.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 711 (1.7%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 639 (1.5%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'total_area_sqm' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 67893 (89.9%)\\n\",\n",
    "      \"  Null values: 7615 (10.1%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 3.00\\n\",\n",
    "      \"  Max: 88140.00\\n\",\n",
    "      \"  Mean: 163.67\\n\",\n",
    "      \"  Median: 127.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 3995 (5.9%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 111 (0.2%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'surface_land_sqm' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 39255 (52.0%)\\n\",\n",
    "      \"  Null values: 36253 (48.0%)\\n\",\n",
    "      \"  Data type: float64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 950774.00\\n\",\n",
    "      \"  Mean: 1157.09\\n\",\n",
    "      \"  Median: 362.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 3534 (9.0%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 144 (0.4%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'nbr_frontages' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 49162 (65.1%)\\n\",\n",
    "      \"  Null values: 26346 (34.9%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 1.00\\n\",\n",
    "      \"  Max: 47.00\\n\",\n",
    "      \"  Mean: 2.80\\n\",\n",
    "      \"  Median: 3.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 17 (0.0%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 31 (0.1%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'nbr_bedrooms' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 200.00\\n\",\n",
    "      \"  Mean: 2.79\\n\",\n",
    "      \"  Median: 3.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 8375 (11.1%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 561 (0.7%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'equipped_kitchen' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_furnished' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.02\\n\",\n",
    "      \"  Median: 0.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 1419 (1.9%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 1419 (1.9%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_open_fire' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.17\\n\",\n",
    "      \"  Median: 0.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 12829 (17.0%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_terrace' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.59\\n\",\n",
    "      \"  Median: 1.00\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'terrace_sqm' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 62368 (82.6%)\\n\",\n",
    "      \"  Null values: 13140 (17.4%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 3466.00\\n\",\n",
    "      \"  Mean: 11.58\\n\",\n",
    "      \"  Median: 1.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 4423 (7.1%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 345 (0.6%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_garden' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.22\\n\",\n",
    "      \"  Median: 0.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 16483 (21.8%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'garden_sqm' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 72569 (96.1%)\\n\",\n",
    "      \"  Null values: 2939 (3.9%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 150000.00\\n\",\n",
    "      \"  Mean: 115.65\\n\",\n",
    "      \"  Median: 0.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 13544 (18.7%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 244 (0.3%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_swimming_pool' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: Int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.02\\n\",\n",
    "      \"  Median: 0.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 1411 (1.9%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 1411 (1.9%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_floodzone' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.54\\n\",\n",
    "      \"  Median: 1.00\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'state_building' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'primary_energy_consumption_sqm' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 48942 (64.8%)\\n\",\n",
    "      \"  Null values: 26566 (35.2%)\\n\",\n",
    "      \"  Data type: float64\\n\",\n",
    "      \"  Min: -140.00\\n\",\n",
    "      \"  Max: 20231122.00\\n\",\n",
    "      \"  Mean: 1688.81\\n\",\n",
    "      \"  Median: 242.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 1661 (3.4%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 4 (0.0%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'epc' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'heating_type' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: object\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'fl_double_glazing' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 75508 (100.0%)\\n\",\n",
    "      \"  Null values: 0 (0.0%)\\n\",\n",
    "      \"  Data type: int64\\n\",\n",
    "      \"  Min: 0.00\\n\",\n",
    "      \"  Max: 1.00\\n\",\n",
    "      \"  Mean: 0.68\\n\",\n",
    "      \"  Median: 1.00\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Statistics for 'cadastral_income' ---\\n\",\n",
    "      \"  Total rows: 75508\\n\",\n",
    "      \"  Non-null values: 30544 (40.5%)\\n\",\n",
    "      \"  Null values: 44964 (59.5%)\\n\",\n",
    "      \"  Data type: float64\\n\",\n",
    "      \"  Min: 1.00\\n\",\n",
    "      \"  Max: 17001700.00\\n\",\n",
    "      \"  Mean: 1885.94\\n\",\n",
    "      \"  Median: 850.00\\n\",\n",
    "      \"  ⚠️  Potential outliers (IQR method): 2230 (7.3%)\\n\",\n",
    "      \"  ⚠️  Potential outliers (Z-Score > 3): 6 (0.0%)\\n\",\n",
    "      \"✅ Successfully created new column 'log_price' using np.log1p.\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Multivariate LOF Outlier Detection ---\\n\",\n",
    "      \"  Analysing 43498 rows with features: ['log_price', 'total_area_sqm', 'garden_sqm']\\n\",\n",
    "      \"  ⚠️  Multivariate LOF Outliers: 3818 (8.8%)\\n\",\n",
    "      \"\\n\",\n",
    "      \"Saving cleaned output as csv ...\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 6\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "50cce18d4db33d4b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
