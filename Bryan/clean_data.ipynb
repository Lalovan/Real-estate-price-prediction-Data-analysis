{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleaning data",
   "id": "bc445b4a7cd8fca4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Robust data cleaning!\n",
    "\n",
    "### 1. **Better Type Handling**\n",
    "- Uses `Int64` (nullable integer) instead of to preserve NaN values `int`\n",
    "- Uses `float64` for decimal precision\n",
    "- Properly handles data type conversions\n",
    "\n",
    "### 2. **Comprehensive Data Cleaning**\n",
    "- Handles multiple decimal points\n",
    "- Removes currency symbols (€, $, £, etc.)\n",
    "- Handles European number formats (1.234.567,89)\n",
    "- Removes units (m², sqm, etc.)\n",
    "- Handles various null representations\n",
    "\n",
    "### 3. **Data Validation**\n",
    "- Min/max value constraints\n",
    "- Negative value handling\n",
    "- Outlier detection using IQR method\n",
    "- Comprehensive reporting\n",
    "\n",
    "### 4. **Better Error Handling**\n",
    "- Checks if columns exist\n",
    "- Reports invalid data conversions\n",
    "- Provides detailed statistics\n",
    "\n",
    "### 5. **Production-Ready Features**\n",
    "- Proper documentation\n",
    "- Progress indicators\n",
    "- Data quality checks\n",
    "- Validation summary\n",
    "- File size reporting"
   ],
   "id": "bfdf8bdfe8a8a5cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ENHANCED HELPER FUNCTIONS\n",
    "def clean_numeric_column(df, column, as_int=False, is_price=False, is_percentage=False,\n",
    "                         allow_negative=False, min_value=None, max_value=None):\n",
    "    \"\"\"\n",
    "    Clean and standardize numeric columns with comprehensive data validation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the column to clean\n",
    "    column : str\n",
    "        Name of the column to clean\n",
    "    as_int : bool\n",
    "        Convert to integer type (default: False, keeps as float)\n",
    "    is_price : bool\n",
    "        Special handling for price/currency formats (default: False)\n",
    "    is_percentage : bool\n",
    "        Special handling for percentage values (default: False)\n",
    "    allow_negative : bool\n",
    "        Whether to allow negative values (default: False)\n",
    "    min_value : float or None\n",
    "        Minimum acceptable value, values below are set to NaN\n",
    "    max_value : float or None\n",
    "        Maximum acceptable value, values above are set to NaN\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with cleaned numeric column\n",
    "    \"\"\"\n",
    "\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column '{column}' not found in dataframe\")\n",
    "        return df\n",
    "\n",
    "    # Store original non-null count for reporting\n",
    "    original_non_null = df[column].notna().sum()\n",
    "\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    cleaned_series = df[column].copy()\n",
    "\n",
    "    # Convert to string for cleaning, preserve NaN\n",
    "    cleaned_series = cleaned_series.astype(str)\n",
    "\n",
    "    # Replace common null representations\n",
    "    null_patterns = ['nan', 'none', 'null', 'n/a', 'na', '#n/a', '#value!', '#ref!',\n",
    "                     '<na>', 'missing', 'MISSING', '--', '']\n",
    "    cleaned_series = cleaned_series.replace(null_patterns, np.nan, regex=False)\n",
    "\n",
    "    # deal with multiple empty spaces\n",
    "    cleaned_series = cleaned_series.str.strip()\n",
    "    cleaned_series.replace('', np.nan, inplace=True)\n",
    "\n",
    "    # Skip if all values are NaN\n",
    "    if cleaned_series.isna().all():\n",
    "        df[column] = np.nan\n",
    "        print(f\"  Warning: Column '{column}' contains only null values\")\n",
    "        return df\n",
    "\n",
    "    # Handle percentage values\n",
    "    if is_percentage:\n",
    "        # Convert percentages like \"85%\", \"85 %\", \"0.85\" to decimal\n",
    "        cleaned_series = cleaned_series.str.replace(r'\\s*%\\s*', '', regex=True)\n",
    "        # Check if values are in percentage format (>1) or decimal format (<=1)\n",
    "        temp_numeric = pd.to_numeric(cleaned_series, errors='coerce')\n",
    "        # If most non-null values are > 1, assume they're percentages\n",
    "        if (temp_numeric > 1).sum() > (temp_numeric <= 1).sum():\n",
    "            temp_numeric = temp_numeric / 100\n",
    "        cleaned_series = temp_numeric\n",
    "\n",
    "    # Handle price/currency values\n",
    "    elif is_price:\n",
    "        # Remove currency symbols: €, $, £, ¥, CHF, USD, EUR, etc.\n",
    "        cleaned_series = cleaned_series.str.replace(r'[€$£¥₹₽¢]', '', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(r'\\b(USD|EUR|GBP|CHF|CAD|AUD|JPY)\\b', '',\n",
    "                                                   regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "        # Handle European number format: 1.234.567,89 -> 1234567.89\n",
    "        # First, remove thousand separators (dots, spaces, apostrophes)\n",
    "        cleaned_series = cleaned_series.str.replace(r'(?<=\\d)[.\\s\\'\\u202f\\u00a0]+(?=\\d{3})', '', regex=True)\n",
    "\n",
    "        # Convert decimal comma to dot: 123,45 -> 123.45\n",
    "        # But only if it's the last comma (to avoid issues with thousand separators)\n",
    "        cleaned_series = cleaned_series.str.replace(r',(\\d{1,2})$', r'.\\1', regex=True)\n",
    "\n",
    "        # Remove any remaining non-numeric characters except decimal point and minus\n",
    "        cleaned_series = cleaned_series.str.replace(r'[^\\d.\\-]', '', regex=True)\n",
    "\n",
    "    # Handle regular numeric values (areas, counts, etc.)\n",
    "    else:\n",
    "        # Remove units like m², m2, sqm, sq.m, cm, km, etc.\n",
    "        cleaned_series = cleaned_series.str.replace(r'\\s*(m[²2]|sqm?|sq\\.?\\s*m|cm|km|ft²?|square\\s*meters?)\\s*',\n",
    "                                                   '', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove thousand separators and normalize decimal separators\n",
    "        cleaned_series = cleaned_series.str.replace(r'(?<=\\d)[.\\s\\'\\u202f\\u00a0]+(?=\\d{3})', '', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(',', '.', regex=False)\n",
    "\n",
    "        # Remove any remaining non-numeric characters except decimal point and minus\n",
    "        cleaned_series = cleaned_series.str.replace(r'[^\\d.\\-]', '', regex=True)\n",
    "\n",
    "    # Handle multiple decimal points (keep only the last one)\n",
    "    def fix_multiple_decimals(val):\n",
    "        if pd.isna(val) or val == '':\n",
    "            return np.nan\n",
    "        if isinstance(val, str) and val.count('.') > 1:\n",
    "            parts = val.rsplit('.', 1)  # Split at the last decimal point\n",
    "            return parts[0].replace('.', '') + '.' + parts[1]\n",
    "        return val\n",
    "\n",
    "    cleaned_series = cleaned_series.apply(fix_multiple_decimals)\n",
    "\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    cleaned_series = pd.to_numeric(cleaned_series, errors='coerce')\n",
    "\n",
    "    # Handle negative values\n",
    "    if not allow_negative:\n",
    "        negative_count = (cleaned_series < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"  Warning: Found {negative_count} negative values in '{column}', setting to NaN\")\n",
    "        cleaned_series = cleaned_series.where(cleaned_series >= 0, np.nan)\n",
    "\n",
    "    # Apply min/max value constraints\n",
    "    if min_value is not None:\n",
    "        out_of_range = (cleaned_series < min_value).sum()\n",
    "        if out_of_range > 0:\n",
    "            print(f\"  Warning: Found {out_of_range} values below minimum ({min_value}) in '{column}'\")\n",
    "        cleaned_series = cleaned_series.where(cleaned_series >= min_value, np.nan)\n",
    "\n",
    "    if max_value is not None:\n",
    "        out_of_range = (cleaned_series > max_value).sum()\n",
    "        if out_of_range > 0:\n",
    "            print(f\"  Warning: Found {out_of_range} values above maximum ({max_value}) in '{column}'\")\n",
    "        cleaned_series = cleaned_series.where(cleaned_series <= max_value, np.nan)\n",
    "\n",
    "    # Convert to appropriate dtype\n",
    "    if as_int:\n",
    "        # Use Int64 (nullable integer) instead of int to preserve NaN\n",
    "        cleaned_series = cleaned_series.astype('Int64')\n",
    "    else:\n",
    "        # Keep as float64 for decimal precision\n",
    "        cleaned_series = cleaned_series.astype('float64')\n",
    "\n",
    "    # Report cleaning results\n",
    "    final_non_null = cleaned_series.notna().sum()\n",
    "    nulls_created = original_non_null - final_non_null\n",
    "    if nulls_created > 0:\n",
    "        print(f\"  Cleaned '{column}': {nulls_created} values converted to null due to invalid data\")\n",
    "\n",
    "    # Assign back to dataframe\n",
    "    df[column] = cleaned_series\n",
    "\n",
    "    return df"
   ],
   "id": "c56bc88ee187da14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Validate with IQR and Z-score",
   "id": "c8c0a8fb8ee60881"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore # We'll use scipy for an easy Z-Score calculation\n",
    "\n",
    "def validate_and_report(df, stats=True):\n",
    "    \"\"\"\n",
    "    Validate and report statistics for a cleaned numeric column, including\n",
    "    outlier detection using IQR and Z-Score methods.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the column\n",
    "    stats : bool\n",
    "        Whether to print detailed statistics\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        col_data = df[column]\n",
    "\n",
    "        print(f\"\\n--- Statistics for '{column}' ---\")\n",
    "        print(f\"  Total rows: {len(col_data)}\")\n",
    "        print(f\"  Non-null values: {col_data.notna().sum()} ({col_data.notna().sum()/len(col_data)*100:.1f}%)\")\n",
    "        print(f\"  Null values: {col_data.isna().sum()} ({col_data.isna().sum()/len(col_data)*100:.1f}%)\")\n",
    "        print(f\"  Data type: {col_data.dtype}\")\n",
    "\n",
    "        # Check if the column is numeric and has non-null data to calculate statistics\n",
    "        if stats and col_data.notna().any() and (col_data.dtype.kind in 'fi'):\n",
    "\n",
    "            # Use only non-null values for statistical calculations\n",
    "            non_null_data = col_data.dropna()\n",
    "\n",
    "            print(f\"  Min: {non_null_data.min():.2f}\")\n",
    "            print(f\"  Max: {non_null_data.max():.2f}\")\n",
    "            print(f\"  Mean: {non_null_data.mean():.2f}\")\n",
    "            print(f\"  Median: {non_null_data.median():.2f}\")\n",
    "\n",
    "            # --- 1. IQR Method ---\n",
    "            Q1 = non_null_data.quantile(0.25)\n",
    "            Q3 = non_null_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers_iqr = ((non_null_data < (Q1 - 1.5 * IQR)) | (non_null_data > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers_iqr > 0:\n",
    "                print(f\"  ⚠️  Potential outliers (IQR method): {outliers_iqr} ({outliers_iqr/len(non_null_data)*100:.1f}%)\")\n",
    "\n",
    "            # --- 2. Z-Score Method ---\n",
    "            # Calculate Z-score for non-null data\n",
    "            z_scores = zscore(non_null_data)\n",
    "            # Outliers are typically defined as those with |Z-score| > 3\n",
    "            outliers_zscore = (np.abs(z_scores) > 3).sum()\n",
    "            if outliers_zscore > 0:\n",
    "                print(f\"  ⚠️  Potential outliers (Z-Score > 3): {outliers_zscore} ({outliers_zscore/len(non_null_data)*100:.1f}%)\")"
   ],
   "id": "eda538ad0ce5c343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reporting with LOF",
   "id": "1b96ebde5f5bc845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def report_lof_outliers(df, features, contamination='auto', n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Reports the number of Local Outlier Factor (LOF) anomalies\n",
    "    on selected numerical features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The full dataset.\n",
    "    features : list\n",
    "        List of numeric column names to use for LOF calculation\n",
    "        (e.g., ['price', 'total_area_sqm', 'nbr_bedrooms']).\n",
    "    contamination : float or 'auto'\n",
    "        The expected proportion of outliers in the dataset.\n",
    "    n_neighbors : int\n",
    "        Number of neighbors to consider for local density.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Multivariate LOF Outlier Detection ---\")\n",
    "\n",
    "    # 1. Prepare data (drop N/A and select features)\n",
    "    df_lof = df[features].dropna().drop_duplicates()\n",
    "    print(f\"  Analysing {len(df_lof)} rows with features: {features}\")\n",
    "\n",
    "    # 2. Initialize and Fit LOF Model\n",
    "    # `novelty=False` is used for outlier detection (vs. novelty detection)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors,\n",
    "                             contamination=contamination,\n",
    "                             novelty=False)\n",
    "\n",
    "    # Predict returns -1 for outliers and 1 for inliers\n",
    "    outlier_labels = lof.fit_predict(df_lof)\n",
    "\n",
    "    # 3. Calculate Results\n",
    "    # Count of LOF outliers (where the prediction is -1)\n",
    "    outliers_lof = (outlier_labels == -1).sum()\n",
    "\n",
    "    # 4. Report\n",
    "    if outliers_lof > 0:\n",
    "        print(f\"  ⚠️  Multivariate LOF Outliers: {outliers_lof} ({outliers_lof/len(df_lof)*100:.1f}%)\")\n",
    "\n",
    "        # Optionally, get the outlier scores\n",
    "        # lof_scores = lof.negative_outlier_factor_\n",
    "        # print(\"  Top 5 LOF scores (most anomalous):\", np.sort(lof_scores)[:5])\n",
    "    else:\n",
    "        print(\"  ✅ No multivariate LOF outliers detected.\")"
   ],
   "id": "db85751d90c1adf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This class should contain all the functions that helpls us clean the data.",
   "id": "a96040a2d145476b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# MAIN CLASS\n",
    "class DataProcessing:\n",
    "    def __init__(self, file_path='../Kristin/sample_data_copy/properties.csv'):\n",
    "        # update line of code above with local CSV file path to load data <---\n",
    "        # ensure old df is cleared so a new file will truly be read (and not a cached file)\n",
    "        if hasattr(self, 'df'):\n",
    "            del self.df\n",
    "        # auto-detect separator in CSV file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            sep = ';' if ';' in first_line else ','  # choose ';' if present, else ','\n",
    "        # load full csv file\n",
    "        self.df = pd.read_csv(file_path, sep=sep, dtype={\"id\": str}, low_memory=False)\n",
    "        print(\"Detected separator:\", repr(sep))\n",
    "        print(\"\\nBefore any cleaning:\")\n",
    "        print(self.df.dtypes,\"\\n\")\n",
    "        print(self.df.head(5))\n",
    "        print(\"\\nNumber of rows raw data loaded:\", len(self.df))\n",
    "\n",
    "    def process_data(self): # main method to process data, further methods detailed below\n",
    "        self.clean_price()\n",
    "        self.clean_areas()\n",
    "        self.convert_yes_no_columns()\n",
    "        self.clean_other_numeric_columns()\n",
    "        self.remove_duplicates()\n",
    "        self.remove_empty_rows()\n",
    "        self.clean_missing()\n",
    "        self.strip_text_columns()\n",
    "\n",
    "    def clean_price(self): # method to clean the price column\n",
    "        if 'price' in self.df.columns:\n",
    "            self.df = clean_numeric_column(self.df, 'price', as_int=True, is_price=True)\n",
    "            print(\"Cleaning price fields...\")\n",
    "\n",
    "    def clean_areas(self): # method to clean the area columns\n",
    "        for col in ['total_area_sqm', 'terrace_sqm', 'garden_sqm']:\n",
    "            if col in self.df.columns:\n",
    "                # Remove units like 'm2', 'm²' (case-insensitive)\n",
    "                self.df[col] = self.df[col].astype(str).str.replace(r'\\s*m[²2]', '', regex=True)\n",
    "                self.df = clean_numeric_column(self.df, col, as_int=True)\n",
    "            print(\"Cleaning area fields...\")\n",
    "\n",
    "    def convert_yes_no_columns(self): # method to convert yes/no to 1/0\n",
    "        \"\"\"\n",
    "        Converts 'yes'/'no' style columns to 1, 0, or pd.NA (for missing data).\n",
    "        The key is using .replace() to handle mapping while preserving original NaNs.\n",
    "        \"\"\"\n",
    "        # Mapping: 'yes'/'y' to 1, 'no'/'n' to 0\n",
    "        yes_no_map = {'yes': 1, 'y': 1, 'no': 0, 'n': 0}\n",
    "\n",
    "        # The columns of interest (assuming these are the correct column names)\n",
    "        for col in ['fl_furnished', 'fl_open_fire', 'fl_swimming_pool']:\n",
    "            if col in self.df.columns:\n",
    "                # 1. Strip and lowercase all non-NaN values\n",
    "                # We use .str.lower() on a Series that is already stripped/lowercased.\n",
    "                # Crucially, we use fillna('') to temporarily convert NaNs to empty string\n",
    "                # so the .str methods can be applied without error, then convert back.\n",
    "\n",
    "                # Use .apply() with a lambda to handle string cleaning while preserving NaN\n",
    "                cleaned_series = self.df[col].apply(lambda x: str(x).strip().lower()\n",
    "                                                    if pd.notna(x) and x is not None else np.nan)\n",
    "\n",
    "                # 2. Use replace() on the cleaned series, which preserves existing NaN values\n",
    "                # Use pd.NA as the final placeholder for unmapped/missing data\n",
    "                self.df[col] = (\n",
    "                    cleaned_series\n",
    "                    .replace(yes_no_map)\n",
    "                    .replace({np.nan: pd.NA}) # Ensure explicit NaN values are pd.NA\n",
    "                    .astype('Int64') # Use nullable integer type to allow pd.NA\n",
    "                )\n",
    "        print(\"Converting Yes/No columns to 1, 0, or Nan integers...\")\n",
    "\n",
    "    def clean_other_numeric_columns(self): # convert other numeric columns to integers\n",
    "        for col in ['nbr_bedrooms', 'nbr_frontages', 'construction_year']:\n",
    "            if col in self.df.columns:\n",
    "                self.df = clean_numeric_column(self.df, col, as_int=True)\n",
    "        print(\"Cleaning other numeric fields...\")\n",
    "\n",
    "    def remove_duplicates(self): # method to remove duplicates based on all columns except id\n",
    "        cols_to_check = [col for col in self.df.columns if col != 'id']\n",
    "        # Find duplicates\n",
    "        duplicates_mask = self.df.duplicated(subset=cols_to_check, keep=False)\n",
    "        num_duplicates = duplicates_mask.sum()\n",
    "        if num_duplicates > 0:\n",
    "            print(f\"\\nFound {num_duplicates} duplicate row(s)\")\n",
    "            # print(self.df[duplicates_mask].sort_values(by=cols_to_check).head(10)) # showing first 10 duplicates\n",
    "        else:\n",
    "            print(\"\\nNo duplicate rows found.\")\n",
    "        self.df.drop_duplicates(subset=cols_to_check, keep='first', inplace=True)\n",
    "        print(\"Removing duplicates...\")\n",
    "        print(f\"Number of rows left after removing duplicates = {len(self.df)}\")\n",
    "\n",
    "    def remove_empty_rows(self): # method to remove rows where id is missing or all other fields are empty\n",
    "        critical_cols = [col for col in self.df.columns if col != 'id']\n",
    "        # identify rows where all non-id columns are empty\n",
    "        # for numeric columns check NaN, for others, check empty string after stripping\n",
    "        empty_mask = pd.Series(True, index=self.df.index)\n",
    "        for col in critical_cols:\n",
    "            if self.df[col].dtype in [int, float]:\n",
    "                col_empty = self.df[col].isna()\n",
    "            else:\n",
    "                col_empty = self.df[col].astype(str).str.strip().eq('') | self.df[col].isna()\n",
    "            empty_mask &= col_empty\n",
    "\n",
    "        missing_id_mask = self.df['id'].isna() | (self.df['id'].astype(str).str.strip() == '') # remove rows without id\n",
    "        num_missing = missing_id_mask.sum()\n",
    "        print(f\"\\nFound {num_missing} row(s) with missing id\")\n",
    "        self.df = self.df.loc[~missing_id_mask] # drop rows with missing id\n",
    "\n",
    "        rows_to_drop = self.df[empty_mask].index # remove rows where all non-id fields are empty\n",
    "        num_empty_rows = len(rows_to_drop)\n",
    "        print(f\"Found {num_empty_rows} row(s) where all non-id fields are empty\")\n",
    "        if num_empty_rows >0:\n",
    "            print(\"Preview of up to 10 rows to be removed (by id):\")\n",
    "            display(self.df.loc[rows_to_drop[:10], :])  # this will print the first 10 rows to be removed\n",
    "\n",
    "        self.df.drop(index=rows_to_drop, inplace=True)\n",
    "        print(\"Removing empty rows...\")\n",
    "        print(f\"Number of rows left after removing empty rows = {len(self.df)}\")\n",
    "\n",
    "    def strip_text_columns(self): # strip leading and trailing spaces from text\n",
    "        text_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in text_cols:\n",
    "            self.df[col] = self.df[col].astype(str).str.strip()\n",
    "        print(\"\\nStripping leading/trailing spaces from all text columns...\")\n",
    "\n",
    "    def clean_missing(self): # method to clean any \"MISSING\" string with NaN and ensure missing fields remain NaN\n",
    "        for col in self.df.columns:\n",
    "            if col != 'id':\n",
    "                # Convert 'MISSING' (case-insensitive) to NaN\n",
    "                self.df[col] = self.df[col].replace(r'(?i)^MISSING$', np.nan, regex=True)\n",
    "                # Also ensure empty strings are treated as NaN\n",
    "                if self.df[col].dtype == 'object':\n",
    "                    self.df[col] = self.df[col].replace(r'^\\s*$', np.nan, regex=True)\n",
    "        print(\"\\nCleaning missing values: 'MISSING' and converting empty strings to NaN...\")\n",
    "\n",
    "    def save_to_csv(self, output_path='../Kristin/cleaned_properties.csv'): # method to create the output file, update file path <---\n",
    "        self.df.to_csv(output_path, index=False)\n",
    "        print(\"\\nSaving cleaned output as csv ...\")"
   ],
   "id": "8e8b43cfbbdd8f79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "log-transforming function for one column",
   "id": "c7f75fded77e8635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def log_transform_column(df: pd.DataFrame, column_name: str, new_col_name: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the natural log transformation (log(1+x)) to a specified column\n",
    "    and adds it as a new column to the DataFrame.\n",
    "\n",
    "    Log transformation is primarily used for features with a high positive skew\n",
    "    (like price or area) to make the distribution more normal.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    column_name : str\n",
    "        The name of the column to be transformed (e.g., 'price').\n",
    "    new_col_name : str, optional\n",
    "        The name of the new log-transformed column.\n",
    "        Defaults to f'log_{column_name}'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with the new log-transformed column added.\n",
    "    \"\"\"\n",
    "    if new_col_name is None:\n",
    "        new_col_name = f'log_{column_name}'\n",
    "\n",
    "    # 1. Check if the column exists\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    # 2. Check for non-positive values (logarithm is undefined for <= 0)\n",
    "    # Since we use log1p, we only check for negative values\n",
    "    if (df[column_name] < 0).any():\n",
    "        print(f\"Warning: Column '{column_name}' contains negative values. \"\n",
    "              \"Applying log transformation to negative numbers is problematic.\")\n",
    "\n",
    "    try:\n",
    "        # 3. Apply the log(1 + x) transformation (log1p)\n",
    "        # We use .copy() to ensure we are operating on a new DataFrame\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # We use .fillna(0) inside log1p to handle any NaN values gracefully\n",
    "        # (they become log(1+0) = 0 in the new column, or NaN if they should remain)\n",
    "        # If NaN should remain NaN after transformation, use .dropna() first or ensure NaNs are skipped.\n",
    "        # Here we apply it directly, NaNs will result in NaNs in the new column.\n",
    "        df_copy[new_col_name] = np.log1p(df_copy[column_name])\n",
    "\n",
    "        print(f\"✅ Successfully created new column '{new_col_name}' using np.log1p.\")\n",
    "        return df_copy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during log transformation: {e}\")\n",
    "        return df"
   ],
   "id": "7b56406a3a615f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dp = DataProcessing(file_path='./sample_data_copy/properties.csv')  # adjust path\n",
    "dp.process_data()\n",
    "\n",
    "validate_and_report(dp.df)\n",
    "\n",
    "log_transformed_price = log_transform_column(dp.df, 'price')\n",
    "report_lof_outliers(log_transformed_price, ['log_price', 'total_area_sqm', 'garden_sqm'])\n",
    "\n",
    "dp.save_to_csv('./cleaned_properties.csv')"
   ],
   "id": "93a88b4f94d17b5a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
