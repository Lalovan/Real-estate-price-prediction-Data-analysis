{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cleaning data",
   "id": "bc445b4a7cd8fca4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Robust data cleaning!\n",
    "\n",
    "### 1. **Better Type Handling**\n",
    "- Uses `Int64` (nullable integer) instead of to preserve NaN values `int`\n",
    "- Uses `float64` for decimal precision\n",
    "- Properly handles data type conversions\n",
    "\n",
    "### 2. **Comprehensive Data Cleaning**\n",
    "- Handles multiple decimal points\n",
    "- Removes currency symbols (€, $, £, etc.)\n",
    "- Handles European number formats (1.234.567,89)\n",
    "- Removes units (m², sqm, etc.)\n",
    "- Handles various null representations\n",
    "\n",
    "### 3. **Data Validation**\n",
    "- Min/max value constraints\n",
    "- Negative value handling\n",
    "- Outlier detection using IQR method\n",
    "- Comprehensive reporting\n",
    "\n",
    "### 4. **Better Error Handling**\n",
    "- Checks if columns exist\n",
    "- Reports invalid data conversions\n",
    "- Provides detailed statistics\n",
    "\n",
    "### 5. **Production-Ready Features**\n",
    "- Proper documentation\n",
    "- Progress indicators\n",
    "- Data quality checks\n",
    "- Validation summary\n",
    "- File size reporting"
   ],
   "id": "bfdf8bdfe8a8a5cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:29.179468Z",
     "start_time": "2025-11-13T15:38:29.149840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ENHANCED HELPER FUNCTIONS\n",
    "def clean_numeric_column(df, column, as_int=False, is_price=False, is_percentage=False,\n",
    "                         allow_negative=False, min_value=None, max_value=None):\n",
    "    \"\"\"\n",
    "    Clean and standardize numeric columns with comprehensive data validation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the column to clean\n",
    "    column : str\n",
    "        Name of the column to clean\n",
    "    as_int : bool\n",
    "        Convert to integer type (default: False, keeps as float)\n",
    "    is_price : bool\n",
    "        Special handling for price/currency formats (default: False)\n",
    "    is_percentage : bool\n",
    "        Special handling for percentage values (default: False)\n",
    "    allow_negative : bool\n",
    "        Whether to allow negative values (default: False)\n",
    "    min_value : float or None\n",
    "        Minimum acceptable value, values below are set to NaN\n",
    "    max_value : float or None\n",
    "        Maximum acceptable value, values above are set to NaN\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with cleaned numeric column\n",
    "    \"\"\"\n",
    "\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column '{column}' not found in dataframe\")\n",
    "        return df\n",
    "\n",
    "    # Store original non-null count for reporting\n",
    "    original_non_null = df[column].notna().sum()\n",
    "\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    cleaned_series = df[column].copy()\n",
    "\n",
    "    # Convert to string for cleaning, preserve NaN\n",
    "    cleaned_series = cleaned_series.astype(str)\n",
    "\n",
    "    # Replace common null representations\n",
    "    null_patterns = ['nan', 'none', 'null', 'n/a', 'na', '#n/a', '#value!', '#ref!',\n",
    "                     '<na>', 'missing', 'MISSING', '--', '']\n",
    "    cleaned_series = cleaned_series.replace(null_patterns, np.nan, regex=False)\n",
    "\n",
    "    # deal with multiple empty spaces\n",
    "    cleaned_series = cleaned_series.str.strip()\n",
    "    cleaned_series.replace('', np.nan, inplace=True)\n",
    "\n",
    "    # Skip if all values are NaN\n",
    "    if cleaned_series.isna().all():\n",
    "        df[column] = np.nan\n",
    "        print(f\"  Warning: Column '{column}' contains only null values\")\n",
    "        return df\n",
    "\n",
    "    # Handle percentage values\n",
    "    if is_percentage:\n",
    "        # Convert percentages like \"85%\", \"85 %\", \"0.85\" to decimal\n",
    "        cleaned_series = cleaned_series.str.replace(r'\\s*%\\s*', '', regex=True)\n",
    "        # Check if values are in percentage format (>1) or decimal format (<=1)\n",
    "        temp_numeric = pd.to_numeric(cleaned_series, errors='coerce')\n",
    "        # If most non-null values are > 1, assume they're percentages\n",
    "        if (temp_numeric > 1).sum() > (temp_numeric <= 1).sum():\n",
    "            temp_numeric = temp_numeric / 100\n",
    "        cleaned_series = temp_numeric\n",
    "\n",
    "    # Handle price/currency values\n",
    "    elif is_price:\n",
    "        # Remove currency symbols: €, $, £, ¥, CHF, USD, EUR, etc.\n",
    "        cleaned_series = cleaned_series.str.replace(r'[€$£¥₹₽¢]', '', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(r'\\b(USD|EUR|GBP|CHF|CAD|AUD|JPY)\\b', '',\n",
    "                                                   regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "        # Handle European number format: 1.234.567,89 -> 1234567.89\n",
    "        # First, remove thousand separators (dots, spaces, apostrophes)\n",
    "        cleaned_series = cleaned_series.str.replace(r'(?<=\\d)[.\\s\\'\\u202f\\u00a0]+(?=\\d{3})', '', regex=True)\n",
    "\n",
    "        # Convert decimal comma to dot: 123,45 -> 123.45\n",
    "        # But only if it's the last comma (to avoid issues with thousand separators)\n",
    "        cleaned_series = cleaned_series.str.replace(r',(\\d{1,2})$', r'.\\1', regex=True)\n",
    "\n",
    "        # Remove any remaining non-numeric characters except decimal point and minus\n",
    "        cleaned_series = cleaned_series.str.replace(r'[^\\d.\\-]', '', regex=True)\n",
    "\n",
    "    # Handle regular numeric values (areas, counts, etc.)\n",
    "    else:\n",
    "        # Remove units like m², m2, sqm, sq.m, cm, km, etc.\n",
    "        cleaned_series = cleaned_series.str.replace(r'\\s*(m[²2]|sqm?|sq\\.?\\s*m|cm|km|ft²?|square\\s*meters?)\\s*',\n",
    "                                                   '', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove thousand separators and normalize decimal separators\n",
    "        cleaned_series = cleaned_series.str.replace(r'(?<=\\d)[.\\s\\'\\u202f\\u00a0]+(?=\\d{3})', '', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(',', '.', regex=False)\n",
    "\n",
    "        # Remove any remaining non-numeric characters except decimal point and minus\n",
    "        cleaned_series = cleaned_series.str.replace(r'[^\\d.\\-]', '', regex=True)\n",
    "\n",
    "    # Handle multiple decimal points (keep only the last one)\n",
    "    def fix_multiple_decimals(val):\n",
    "        if pd.isna(val) or val == '':\n",
    "            return np.nan\n",
    "        if isinstance(val, str) and val.count('.') > 1:\n",
    "            parts = val.rsplit('.', 1)  # Split at the last decimal point\n",
    "            return parts[0].replace('.', '') + '.' + parts[1]\n",
    "        return val\n",
    "\n",
    "    cleaned_series = cleaned_series.apply(fix_multiple_decimals)\n",
    "\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    cleaned_series = pd.to_numeric(cleaned_series, errors='coerce')\n",
    "\n",
    "    # Handle negative values\n",
    "    if not allow_negative:\n",
    "        negative_count = (cleaned_series < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"  Warning: Found {negative_count} negative values in '{column}', setting to NaN\")\n",
    "        cleaned_series = cleaned_series.where(cleaned_series >= 0, np.nan)\n",
    "\n",
    "    # Apply min/max value constraints\n",
    "    if min_value is not None:\n",
    "        out_of_range = (cleaned_series < min_value).sum()\n",
    "        if out_of_range > 0:\n",
    "            print(f\"  Warning: Found {out_of_range} values below minimum ({min_value}) in '{column}'\")\n",
    "        cleaned_series = cleaned_series.where(cleaned_series >= min_value, np.nan)\n",
    "\n",
    "    if max_value is not None:\n",
    "        out_of_range = (cleaned_series > max_value).sum()\n",
    "        if out_of_range > 0:\n",
    "            print(f\"  Warning: Found {out_of_range} values above maximum ({max_value}) in '{column}'\")\n",
    "        cleaned_series = cleaned_series.where(cleaned_series <= max_value, np.nan)\n",
    "\n",
    "    # Convert to appropriate dtype\n",
    "    if as_int:\n",
    "        # Use Int64 (nullable integer) instead of int to preserve NaN\n",
    "        cleaned_series = cleaned_series.astype('Int64')\n",
    "    else:\n",
    "        # Keep as float64 for decimal precision\n",
    "        cleaned_series = cleaned_series.astype('float64')\n",
    "\n",
    "    # Report cleaning results\n",
    "    final_non_null = cleaned_series.notna().sum()\n",
    "    nulls_created = original_non_null - final_non_null\n",
    "    if nulls_created > 0:\n",
    "        print(f\"  Cleaned '{column}': {nulls_created} values converted to null due to invalid data\")\n",
    "\n",
    "    # Assign back to dataframe\n",
    "    df[column] = cleaned_series\n",
    "\n",
    "    return df"
   ],
   "id": "c56bc88ee187da14",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Validate with IQR and Z-score",
   "id": "c8c0a8fb8ee60881"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:29.248156Z",
     "start_time": "2025-11-13T15:38:29.227136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore # We'll use scipy for an easy Z-Score calculation\n",
    "\n",
    "def validate_and_report(df, stats=True):\n",
    "    \"\"\"\n",
    "    Validate and report statistics for a cleaned numeric column, including\n",
    "    outlier detection using IQR and Z-Score methods.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the column\n",
    "    stats : bool\n",
    "        Whether to print detailed statistics\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        col_data = df[column]\n",
    "\n",
    "        print(f\"\\n--- Statistics for '{column}' ---\")\n",
    "        print(f\"  Total rows: {len(col_data)}\")\n",
    "        print(f\"  Non-null values: {col_data.notna().sum()} ({col_data.notna().sum()/len(col_data)*100:.1f}%)\")\n",
    "        print(f\"  Null values: {col_data.isna().sum()} ({col_data.isna().sum()/len(col_data)*100:.1f}%)\")\n",
    "        print(f\"  Data type: {col_data.dtype}\")\n",
    "\n",
    "        # Check if the column is numeric and has non-null data to calculate statistics\n",
    "        if stats and col_data.notna().any() and (col_data.dtype.kind in 'fi'):\n",
    "\n",
    "            # Use only non-null values for statistical calculations\n",
    "            non_null_data = col_data.dropna()\n",
    "\n",
    "            print(f\"  Min: {non_null_data.min():.2f}\")\n",
    "            print(f\"  Max: {non_null_data.max():.2f}\")\n",
    "            print(f\"  Mean: {non_null_data.mean():.2f}\")\n",
    "            print(f\"  Median: {non_null_data.median():.2f}\")\n",
    "\n",
    "            # --- 1. IQR Method ---\n",
    "            Q1 = non_null_data.quantile(0.25)\n",
    "            Q3 = non_null_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers_iqr = ((non_null_data < (Q1 - 1.5 * IQR)) | (non_null_data > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers_iqr > 0:\n",
    "                print(f\"  ⚠️  Potential outliers (IQR method): {outliers_iqr} ({outliers_iqr/len(non_null_data)*100:.1f}%)\")\n",
    "\n",
    "            # --- 2. Z-Score Method ---\n",
    "            # Calculate Z-score for non-null data\n",
    "            z_scores = zscore(non_null_data)\n",
    "            # Outliers are typically defined as those with |Z-score| > 3\n",
    "            outliers_zscore = (np.abs(z_scores) > 3).sum()\n",
    "            if outliers_zscore > 0:\n",
    "                print(f\"  ⚠️  Potential outliers (Z-Score > 3): {outliers_zscore} ({outliers_zscore/len(non_null_data)*100:.1f}%)\")"
   ],
   "id": "eda538ad0ce5c343",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reporting with LOF",
   "id": "1b96ebde5f5bc845"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:29.958509Z",
     "start_time": "2025-11-13T15:38:29.262528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def report_lof_outliers(df, features, contamination='auto', n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Reports the number of Local Outlier Factor (LOF) anomalies\n",
    "    on selected numerical features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The full dataset.\n",
    "    features : list\n",
    "        List of numeric column names to use for LOF calculation\n",
    "        (e.g., ['price', 'total_area_sqm', 'nbr_bedrooms']).\n",
    "    contamination : float or 'auto'\n",
    "        The expected proportion of outliers in the dataset.\n",
    "    n_neighbors : int\n",
    "        Number of neighbors to consider for local density.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Multivariate LOF Outlier Detection ---\")\n",
    "\n",
    "    # 1. Prepare data (drop N/A and select features)\n",
    "    df_lof = df[features].dropna().drop_duplicates()\n",
    "    print(f\"  Analysing {len(df_lof)} rows with features: {features}\")\n",
    "\n",
    "    # 2. Initialize and Fit LOF Model\n",
    "    # `novelty=False` is used for outlier detection (vs. novelty detection)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors,\n",
    "                             contamination=contamination,\n",
    "                             novelty=False)\n",
    "\n",
    "    # Predict returns -1 for outliers and 1 for inliers\n",
    "    outlier_labels = lof.fit_predict(df_lof)\n",
    "\n",
    "    # 3. Calculate Results\n",
    "    # Count of LOF outliers (where the prediction is -1)\n",
    "    outliers_lof = (outlier_labels == -1).sum()\n",
    "\n",
    "    # 4. Report\n",
    "    if outliers_lof > 0:\n",
    "        print(f\"  ⚠️  Multivariate LOF Outliers: {outliers_lof} ({outliers_lof/len(df_lof)*100:.1f}%)\")\n",
    "\n",
    "        # Optionally, get the outlier scores\n",
    "        # lof_scores = lof.negative_outlier_factor_\n",
    "        # print(\"  Top 5 LOF scores (most anomalous):\", np.sort(lof_scores)[:5])\n",
    "    else:\n",
    "        print(\"  ✅ No multivariate LOF outliers detected.\")"
   ],
   "id": "db85751d90c1adf5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This class should contain all the functions that helpls us clean the data.",
   "id": "a96040a2d145476b"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:30.009585Z",
     "start_time": "2025-11-13T15:38:29.978715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# MAIN CLASS\n",
    "class DataProcessing:\n",
    "    def __init__(self, file_path='../Kristin/sample_data_copy/properties.csv'):\n",
    "        # update line of code above with local CSV file path to load data <---\n",
    "        # ensure old df is cleared so a new file will truly be read (and not a cached file)\n",
    "        if hasattr(self, 'df'):\n",
    "            del self.df\n",
    "        # auto-detect separator in CSV file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            sep = ';' if ';' in first_line else ','  # choose ';' if present, else ','\n",
    "        # load full csv file\n",
    "        self.df = pd.read_csv(file_path, sep=sep, dtype={\"id\": str}, low_memory=False)\n",
    "        print(\"Detected separator:\", repr(sep))\n",
    "        print(\"\\nBefore any cleaning:\")\n",
    "        print(self.df.dtypes,\"\\n\")\n",
    "        print(self.df.head(5))\n",
    "        print(\"\\nNumber of rows raw data loaded:\", len(self.df))\n",
    "\n",
    "    def process_data(self): # main method to process data, further methods detailed below\n",
    "        self.clean_price()\n",
    "        self.clean_areas()\n",
    "        self.convert_yes_no_columns()\n",
    "        self.clean_other_numeric_columns()\n",
    "        self.remove_duplicates()\n",
    "        self.remove_empty_rows()\n",
    "        self.clean_missing()\n",
    "        self.strip_text_columns()\n",
    "\n",
    "    def clean_price(self): # method to clean the price column\n",
    "        if 'price' in self.df.columns:\n",
    "            self.df = clean_numeric_column(self.df, 'price', as_int=True, is_price=True, max_value=3_000_000)\n",
    "            print(\"Cleaning price fields...\")\n",
    "\n",
    "    def clean_areas(self): # method to clean the area columns\n",
    "        for col in ['total_area_sqm', 'terrace_sqm', 'garden_sqm']:\n",
    "            if col in self.df.columns:\n",
    "                # Remove units like 'm2', 'm²' (case-insensitive)\n",
    "                self.df[col] = self.df[col].astype(str).str.replace(r'\\s*m[²2]', '', regex=True)\n",
    "                self.df = clean_numeric_column(self.df, col, as_int=True, max_value=900)\n",
    "            print(\"Cleaning area fields...\")\n",
    "\n",
    "    def convert_yes_no_columns(self): # method to convert yes/no to 1/0\n",
    "        yes_no_map = {'yes': 1, 'y': 1, 'no': 0, 'n': 0}\n",
    "        for col in ['fl_furnished', 'open_fire', 'fl_swimming_pool']:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = (\n",
    "                    self.df[col]\n",
    "                    .astype(str)\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .map(yes_no_map)\n",
    "                    .fillna(0)\n",
    "                    .astype(int)\n",
    "                )\n",
    "        print(\"Converting Yes/No columns to 1/0 integers...\")\n",
    "\n",
    "    def clean_other_numeric_columns(self): # convert other numeric columns to integers\n",
    "        for col in ['nbr_bedrooms', 'nbr_frontages', 'construction_year']:\n",
    "            if col in self.df.columns:\n",
    "                self.df = clean_numeric_column(self.df, col, as_int=True)\n",
    "        print(\"Cleaning other numeric fields...\")\n",
    "\n",
    "    def remove_duplicates(self): # method to remove duplicates based on all columns except id\n",
    "        cols_to_check = [col for col in self.df.columns if col != 'id']\n",
    "        # Find duplicates\n",
    "        duplicates_mask = self.df.duplicated(subset=cols_to_check, keep=False)\n",
    "        num_duplicates = duplicates_mask.sum()\n",
    "        if num_duplicates > 0:\n",
    "            print(f\"\\nFound {num_duplicates} duplicate row(s)\")\n",
    "            # print(self.df[duplicates_mask].sort_values(by=cols_to_check).head(10)) # showing first 10 duplicates\n",
    "        else:\n",
    "            print(\"\\nNo duplicate rows found.\")\n",
    "        self.df.drop_duplicates(subset=cols_to_check, keep='first', inplace=True)\n",
    "        print(\"Removing duplicates...\")\n",
    "        print(f\"Number of rows left after removing duplicates = {len(self.df)}\")\n",
    "\n",
    "    def remove_empty_rows(self): # method to remove rows where id is missing or all other fields are empty\n",
    "        critical_cols = [col for col in self.df.columns if col != 'id']\n",
    "        # identify rows where all non-id columns are empty\n",
    "        # for numeric columns check NaN, for others, check empty string after stripping\n",
    "        empty_mask = pd.Series(True, index=self.df.index)\n",
    "        for col in critical_cols:\n",
    "            if self.df[col].dtype in [int, float]:\n",
    "                col_empty = self.df[col].isna()\n",
    "            else:\n",
    "                col_empty = self.df[col].astype(str).str.strip().eq('') | self.df[col].isna()\n",
    "            empty_mask &= col_empty\n",
    "\n",
    "        missing_id_mask = self.df['id'].isna() | (self.df['id'].astype(str).str.strip() == '') # remove rows without id\n",
    "        num_missing = missing_id_mask.sum()\n",
    "        print(f\"\\nFound {num_missing} row(s) with missing id\")\n",
    "        self.df = self.df.loc[~missing_id_mask] # drop rows with missing id\n",
    "\n",
    "        rows_to_drop = self.df[empty_mask].index # remove rows where all non-id fields are empty\n",
    "        num_empty_rows = len(rows_to_drop)\n",
    "        print(f\"Found {num_empty_rows} row(s) where all non-id fields are empty\")\n",
    "        if num_empty_rows >0:\n",
    "            print(\"Preview of up to 10 rows to be removed (by id):\")\n",
    "            display(self.df.loc[rows_to_drop[:10], :])  # this will print the first 10 rows to be removed\n",
    "\n",
    "        self.df.drop(index=rows_to_drop, inplace=True)\n",
    "        print(\"Removing empty rows...\")\n",
    "        print(f\"Number of rows left after removing empty rows = {len(self.df)}\")\n",
    "\n",
    "    def strip_text_columns(self): # strip leading and trailing spaces from text\n",
    "        text_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in text_cols:\n",
    "            self.df[col] = self.df[col].astype(str).str.strip()\n",
    "        print(\"\\nStripping leading/trailing spaces from all text columns...\")\n",
    "\n",
    "    def clean_missing(self): # method to clean any \"MISSING\" string with NaN and ensure missing fields remain NaN\n",
    "        for col in self.df.columns:\n",
    "            if col != 'id':\n",
    "                # Convert 'MISSING' (case-insensitive) to NaN\n",
    "                self.df[col] = self.df[col].replace(r'(?i)^MISSING$', np.nan, regex=True)\n",
    "                # Also ensure empty strings are treated as NaN\n",
    "                if self.df[col].dtype == 'object':\n",
    "                    self.df[col] = self.df[col].replace(r'^\\s*$', np.nan, regex=True)\n",
    "        print(\"\\nCleaning missing values: 'MISSING' and converting empty strings to NaN...\")\n",
    "\n",
    "    def save_to_csv(self, output_path='../Kristin/cleaned_properties.csv'): # method to create the output file, update file path <---\n",
    "        self.df.to_csv(output_path, index=False)\n",
    "        print(\"\\nSaving cleaned output as csv ...\")"
   ],
   "id": "8e8b43cfbbdd8f79",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "log-transforming function for one column",
   "id": "c7f75fded77e8635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:30.032853Z",
     "start_time": "2025-11-13T15:38:30.022467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def log_transform_column(df: pd.DataFrame, column_name: str, new_col_name: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the natural log transformation (log(1+x)) to a specified column\n",
    "    and adds it as a new column to the DataFrame.\n",
    "\n",
    "    Log transformation is primarily used for features with a high positive skew\n",
    "    (like price or area) to make the distribution more normal.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    column_name : str\n",
    "        The name of the column to be transformed (e.g., 'price').\n",
    "    new_col_name : str, optional\n",
    "        The name of the new log-transformed column.\n",
    "        Defaults to f'log_{column_name}'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with the new log-transformed column added.\n",
    "    \"\"\"\n",
    "    if new_col_name is None:\n",
    "        new_col_name = f'log_{column_name}'\n",
    "\n",
    "    # 1. Check if the column exists\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    # 2. Check for non-positive values (logarithm is undefined for <= 0)\n",
    "    # Since we use log1p, we only check for negative values\n",
    "    if (df[column_name] < 0).any():\n",
    "        print(f\"Warning: Column '{column_name}' contains negative values. \"\n",
    "              \"Applying log transformation to negative numbers is problematic.\")\n",
    "\n",
    "    try:\n",
    "        # 3. Apply the log(1 + x) transformation (log1p)\n",
    "        # We use .copy() to ensure we are operating on a new DataFrame\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # We use .fillna(0) inside log1p to handle any NaN values gracefully\n",
    "        # (they become log(1+0) = 0 in the new column, or NaN if they should remain)\n",
    "        # If NaN should remain NaN after transformation, use .dropna() first or ensure NaNs are skipped.\n",
    "        # Here we apply it directly, NaNs will result in NaNs in the new column.\n",
    "        df_copy[new_col_name] = np.log1p(df_copy[column_name])\n",
    "\n",
    "        print(f\"✅ Successfully created new column '{new_col_name}' using np.log1p.\")\n",
    "        return df_copy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during log transformation: {e}\")\n",
    "        return df"
   ],
   "id": "7b56406a3a615f7d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:44.813977Z",
     "start_time": "2025-11-13T15:38:30.070276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dp = DataProcessing(file_path='../Bryan/sample_data_copy/properties.csv')  # adjust path\n",
    "dp.process_data()\n",
    "\n",
    "validate_and_report(dp.df)\n",
    "\n",
    "log_transformed_price = log_transform_column(dp.df, 'price')\n",
    "report_lof_outliers(log_transformed_price, ['log_price', 'total_area_sqm', 'garden_sqm'])\n",
    "\n",
    "dp.save_to_csv('../Bryan/cleaned_properties.csv')"
   ],
   "id": "93a88b4f94d17b5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected separator: ','\n",
      "\n",
      "Before any cleaning:\n",
      "id                                 object\n",
      "price                             float64\n",
      "property_type                      object\n",
      "subproperty_type                   object\n",
      "region                             object\n",
      "province                           object\n",
      "locality                           object\n",
      "zip_code                            int64\n",
      "latitude                          float64\n",
      "longitude                         float64\n",
      "construction_year                 float64\n",
      "total_area_sqm                    float64\n",
      "surface_land_sqm                  float64\n",
      "nbr_frontages                     float64\n",
      "nbr_bedrooms                      float64\n",
      "equipped_kitchen                   object\n",
      "fl_furnished                        int64\n",
      "fl_open_fire                        int64\n",
      "fl_terrace                          int64\n",
      "terrace_sqm                       float64\n",
      "fl_garden                           int64\n",
      "garden_sqm                        float64\n",
      "fl_swimming_pool                    int64\n",
      "fl_floodzone                        int64\n",
      "state_building                     object\n",
      "primary_energy_consumption_sqm    float64\n",
      "epc                                object\n",
      "heating_type                       object\n",
      "fl_double_glazing                   int64\n",
      "cadastral_income                  float64\n",
      "dtype: object \n",
      "\n",
      "         id     price property_type subproperty_type            region  \\\n",
      "0  34221000  225000.0     APARTMENT        APARTMENT          Flanders   \n",
      "1   2104000  449000.0         HOUSE            HOUSE          Flanders   \n",
      "2  34036000  335000.0     APARTMENT        APARTMENT  Brussels-Capital   \n",
      "3  58496000  501000.0         HOUSE            HOUSE          Flanders   \n",
      "4  48727000  982700.0     APARTMENT           DUPLEX          Wallonia   \n",
      "\n",
      "          province  locality  zip_code   latitude  longitude  ...  fl_garden  \\\n",
      "0          Antwerp   Antwerp      2050  51.217172   4.379982  ...          0   \n",
      "1    East Flanders      Gent      9185  51.174944   3.845248  ...          0   \n",
      "2         Brussels  Brussels      1070  50.842043   4.334543  ...          0   \n",
      "3          Antwerp  Turnhout      2275  51.238312   4.817192  ...          0   \n",
      "4  Walloon Brabant  Nivelles      1410        NaN        NaN  ...          1   \n",
      "\n",
      "   garden_sqm  fl_swimming_pool  fl_floodzone  state_building  \\\n",
      "0         0.0                 0             0         MISSING   \n",
      "1         0.0                 0             0         MISSING   \n",
      "2         0.0                 0             1          AS_NEW   \n",
      "3         0.0                 0             1         MISSING   \n",
      "4       142.0                 0             0          AS_NEW   \n",
      "\n",
      "  primary_energy_consumption_sqm      epc  heating_type  fl_double_glazing  \\\n",
      "0                          231.0        C           GAS                  1   \n",
      "1                          221.0        C       MISSING                  1   \n",
      "2                            NaN  MISSING           GAS                  0   \n",
      "3                           99.0        A       MISSING                  0   \n",
      "4                           19.0       A+           GAS                  0   \n",
      "\n",
      "   cadastral_income  \n",
      "0             922.0  \n",
      "1             406.0  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Number of rows raw data loaded: 75511\n",
      "  Warning: Found 334 values above maximum (3000000) in 'price'\n",
      "  Cleaned 'price': 334 values converted to null due to invalid data\n",
      "Cleaning price fields...\n",
      "  Warning: Found 334 values above maximum (900) in 'total_area_sqm'\n",
      "  Cleaned 'total_area_sqm': 7949 values converted to null due to invalid data\n",
      "Cleaning area fields...\n",
      "  Warning: Found 11 values above maximum (900) in 'terrace_sqm'\n",
      "  Cleaned 'terrace_sqm': 13151 values converted to null due to invalid data\n",
      "Cleaning area fields...\n",
      "  Warning: Found 1670 values above maximum (900) in 'garden_sqm'\n",
      "  Cleaned 'garden_sqm': 4609 values converted to null due to invalid data\n",
      "Cleaning area fields...\n",
      "Converting Yes/No columns to 1/0 integers...\n",
      "Cleaning other numeric fields...\n",
      "\n",
      "Found 14 duplicate row(s)\n",
      "Removing duplicates...\n",
      "Number of rows left after removing duplicates = 75504\n",
      "\n",
      "Found 0 row(s) with missing id\n",
      "Found 0 row(s) where all non-id fields are empty\n",
      "Removing empty rows...\n",
      "Number of rows left after removing empty rows = 75504\n",
      "\n",
      "Cleaning missing values: 'MISSING' and converting empty strings to NaN...\n",
      "\n",
      "Stripping leading/trailing spaces from all text columns...\n",
      "\n",
      "--- Statistics for 'id' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'price' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75170 (99.6%)\n",
      "  Null values: 334 (0.4%)\n",
      "  Data type: Int64\n",
      "  Min: 76000.00\n",
      "  Max: 3000000.00\n",
      "  Mean: 403757.39\n",
      "  Median: 327309.50\n",
      "  ⚠️  Potential outliers (IQR method): 5535 (7.4%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 1708 (2.3%)\n",
      "\n",
      "--- Statistics for 'property_type' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'subproperty_type' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'region' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'province' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'locality' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'zip_code' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 1000.00\n",
      "  Max: 9992.00\n",
      "  Mean: 5144.68\n",
      "  Median: 4683.00\n",
      "\n",
      "--- Statistics for 'latitude' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 61407 (81.3%)\n",
      "  Null values: 14097 (18.7%)\n",
      "  Data type: float64\n",
      "  Min: 25.76\n",
      "  Max: 52.43\n",
      "  Mean: 50.89\n",
      "  Median: 50.90\n",
      "  ⚠️  Potential outliers (IQR method): 1302 (2.1%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 799 (1.3%)\n",
      "\n",
      "--- Statistics for 'longitude' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 61407 (81.3%)\n",
      "  Null values: 14097 (18.7%)\n",
      "  Data type: float64\n",
      "  Min: -80.19\n",
      "  Max: 6.39\n",
      "  Mean: 4.33\n",
      "  Median: 4.38\n",
      "  ⚠️  Potential outliers (IQR method): 2 (0.0%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 2 (0.0%)\n",
      "\n",
      "--- Statistics for 'construction_year' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 42118 (55.8%)\n",
      "  Null values: 33386 (44.2%)\n",
      "  Data type: Int64\n",
      "  Min: 1753.00\n",
      "  Max: 2024.00\n",
      "  Mean: 1984.41\n",
      "  Median: 1994.00\n",
      "  ⚠️  Potential outliers (IQR method): 711 (1.7%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 639 (1.5%)\n",
      "\n",
      "--- Statistics for 'total_area_sqm' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 67555 (89.5%)\n",
      "  Null values: 7949 (10.5%)\n",
      "  Data type: Int64\n",
      "  Min: 3.00\n",
      "  Max: 900.00\n",
      "  Mean: 153.43\n",
      "  Median: 127.00\n",
      "  ⚠️  Potential outliers (IQR method): 3753 (5.6%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 1381 (2.0%)\n",
      "\n",
      "--- Statistics for 'surface_land_sqm' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 39254 (52.0%)\n",
      "  Null values: 36250 (48.0%)\n",
      "  Data type: float64\n",
      "  Min: 0.00\n",
      "  Max: 950774.00\n",
      "  Mean: 1157.11\n",
      "  Median: 362.00\n",
      "  ⚠️  Potential outliers (IQR method): 3534 (9.0%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 144 (0.4%)\n",
      "\n",
      "--- Statistics for 'nbr_frontages' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 49159 (65.1%)\n",
      "  Null values: 26345 (34.9%)\n",
      "  Data type: Int64\n",
      "  Min: 1.00\n",
      "  Max: 47.00\n",
      "  Mean: 2.80\n",
      "  Median: 3.00\n",
      "  ⚠️  Potential outliers (IQR method): 17 (0.0%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 31 (0.1%)\n",
      "\n",
      "--- Statistics for 'nbr_bedrooms' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: Int64\n",
      "  Min: 0.00\n",
      "  Max: 200.00\n",
      "  Mean: 2.79\n",
      "  Median: 3.00\n",
      "  ⚠️  Potential outliers (IQR method): 8375 (11.1%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 561 (0.7%)\n",
      "\n",
      "--- Statistics for 'equipped_kitchen' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'fl_furnished' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 0.00\n",
      "  Mean: 0.00\n",
      "  Median: 0.00\n",
      "\n",
      "--- Statistics for 'fl_open_fire' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 1.00\n",
      "  Mean: 0.17\n",
      "  Median: 0.00\n",
      "  ⚠️  Potential outliers (IQR method): 12828 (17.0%)\n",
      "\n",
      "--- Statistics for 'fl_terrace' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 1.00\n",
      "  Mean: 0.59\n",
      "  Median: 1.00\n",
      "\n",
      "--- Statistics for 'terrace_sqm' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 62353 (82.6%)\n",
      "  Null values: 13151 (17.4%)\n",
      "  Data type: Int64\n",
      "  Min: 0.00\n",
      "  Max: 781.00\n",
      "  Mean: 11.20\n",
      "  Median: 1.00\n",
      "  ⚠️  Potential outliers (IQR method): 4412 (7.1%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 931 (1.5%)\n",
      "\n",
      "--- Statistics for 'fl_garden' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 1.00\n",
      "  Mean: 0.22\n",
      "  Median: 0.00\n",
      "  ⚠️  Potential outliers (IQR method): 16483 (21.8%)\n",
      "\n",
      "--- Statistics for 'garden_sqm' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 70895 (93.9%)\n",
      "  Null values: 4609 (6.1%)\n",
      "  Data type: Int64\n",
      "  Min: 0.00\n",
      "  Max: 900.00\n",
      "  Mean: 34.48\n",
      "  Median: 0.00\n",
      "  ⚠️  Potential outliers (IQR method): 11874 (16.7%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 2285 (3.2%)\n",
      "\n",
      "--- Statistics for 'fl_swimming_pool' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 0.00\n",
      "  Mean: 0.00\n",
      "  Median: 0.00\n",
      "\n",
      "--- Statistics for 'fl_floodzone' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 1.00\n",
      "  Mean: 0.54\n",
      "  Median: 1.00\n",
      "\n",
      "--- Statistics for 'state_building' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'primary_energy_consumption_sqm' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 48939 (64.8%)\n",
      "  Null values: 26565 (35.2%)\n",
      "  Data type: float64\n",
      "  Min: -140.00\n",
      "  Max: 20231122.00\n",
      "  Mean: 1688.91\n",
      "  Median: 242.00\n",
      "  ⚠️  Potential outliers (IQR method): 1661 (3.4%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 4 (0.0%)\n",
      "\n",
      "--- Statistics for 'epc' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'heating_type' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: object\n",
      "\n",
      "--- Statistics for 'fl_double_glazing' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 75504 (100.0%)\n",
      "  Null values: 0 (0.0%)\n",
      "  Data type: int64\n",
      "  Min: 0.00\n",
      "  Max: 1.00\n",
      "  Mean: 0.68\n",
      "  Median: 1.00\n",
      "\n",
      "--- Statistics for 'cadastral_income' ---\n",
      "  Total rows: 75504\n",
      "  Non-null values: 30544 (40.5%)\n",
      "  Null values: 44960 (59.5%)\n",
      "  Data type: float64\n",
      "  Min: 1.00\n",
      "  Max: 17001700.00\n",
      "  Mean: 1885.94\n",
      "  Median: 850.00\n",
      "  ⚠️  Potential outliers (IQR method): 2230 (7.3%)\n",
      "  ⚠️  Potential outliers (Z-Score > 3): 6 (0.0%)\n",
      "✅ Successfully created new column 'log_price' using np.log1p.\n",
      "\n",
      "--- Multivariate LOF Outlier Detection ---\n",
      "  Analysing 41619 rows with features: ['log_price', 'total_area_sqm', 'garden_sqm']\n",
      "  ⚠️  Multivariate LOF Outliers: 3647 (8.8%)\n",
      "\n",
      "Saving cleaned output as csv ...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bagplot of the bivariate",
   "id": "27d3790468c02a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:38:45.180785Z",
     "start_time": "2025-11-13T15:38:45.147637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_bivariate_outliers(df: pd.DataFrame, x_col: str, y_col: str, title: str):\n",
    "    \"\"\"\n",
    "    Creates a bivariate scatter plot with density contours to visualize\n",
    "    the central mass and identify potential outliers (similar to a Bagplot).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    x_col : str\n",
    "        The column name for the X-axis (e.g., 'total_area_sqm').\n",
    "    y_col : str\n",
    "        The column name for the Y-axis (e.g., 'log_price').\n",
    "    title : str\n",
    "        The title for the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop rows with NaN in the selected columns for clean plotting\n",
    "    plot_df = df.dropna(subset=[x_col, y_col]).copy()\n",
    "\n",
    "    # --- 1. Identify Outliers using Bivariate IQR-style boundaries ---\n",
    "    # This is a simple, robust proxy for multivariate outliers:\n",
    "    # flag points that are extreme in *either* dimension.\n",
    "\n",
    "    # Calculate IQR boundaries for X-axis\n",
    "    Q1_x = plot_df[x_col].quantile(0.25)\n",
    "    Q3_x = plot_df[x_col].quantile(0.75)\n",
    "    IQR_x = Q3_x - Q1_x\n",
    "    lower_x = Q1_x - 1.5 * IQR_x\n",
    "    upper_x = Q3_x + 1.5 * IQR_x\n",
    "\n",
    "    # Calculate IQR boundaries for Y-axis\n",
    "    Q1_y = plot_df[y_col].quantile(0.25)\n",
    "    Q3_y = plot_df[y_col].quantile(0.75)\n",
    "    IQR_y = Q3_y - Q1_y\n",
    "    lower_y = Q1_y - 1.5 * IQR_y\n",
    "    upper_y = Q3_y + 1.5 * IQR_y\n",
    "\n",
    "    # Flag outliers: points outside the 1.5*IQR range in EITHER dimension\n",
    "    is_outlier = (\n",
    "        (plot_df[x_col] < lower_x) | (plot_df[x_col] > upper_x) |\n",
    "        (plot_df[y_col] < lower_y) | (plot_df[y_col] > upper_y)\n",
    "    )\n",
    "\n",
    "    plot_df['is_outlier'] = is_outlier\n",
    "\n",
    "    # Separate data for plotting\n",
    "    outliers = plot_df[plot_df['is_outlier']]\n",
    "    inliers = plot_df[~plot_df['is_outlier']]\n",
    "\n",
    "    # --- 2. Create the Visualization ---\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # A. Density Contours (The \"Bag\" and \"Fence\" proxy)\n",
    "    # kdeplot shows where the data is most dense\n",
    "    sns.kdeplot(\n",
    "        x=inliers[x_col],\n",
    "        y=inliers[y_col],\n",
    "        fill=True,\n",
    "        thresh=0.05,\n",
    "        cmap=\"Blues\",\n",
    "        alpha=0.6,\n",
    "        # REMOVED linewidths=0 to suppress the harmless warning\n",
    "        label=\"Central Mass Density\"\n",
    "    )\n",
    "\n",
    "    # B. Inliers (The main body of the data)\n",
    "    sns.scatterplot(\n",
    "        x=inliers[x_col],\n",
    "        y=inliers[y_col],\n",
    "        color='gray',\n",
    "        s=20,\n",
    "        alpha=0.7,\n",
    "        label=f\"Inliers ({len(inliers)})\"\n",
    "    )\n",
    "\n",
    "    # C. Outliers (The extreme points)\n",
    "    sns.scatterplot(\n",
    "        x=outliers[x_col],\n",
    "        y=outliers[y_col],\n",
    "        color='red',\n",
    "        s=50,\n",
    "        marker='X',\n",
    "        label=f\"Outliers ({len(outliers)})\"\n",
    "    )\n",
    "\n",
    "    plt.axvline(upper_x, color='lightcoral', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(upper_y, color='lightcoral', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(x_col, fontsize=12)\n",
    "    plt.ylabel(y_col, fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.show()"
   ],
   "id": "6d44492d064d2cdd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T15:38:45.221078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_bivariate_outliers(\n",
    "    log_transformed_price,\n",
    "    x_col='total_area_sqm',\n",
    "    y_col='log_price',\n",
    "    title='Bivariate Distribution of Area vs. Log(Price) with Outlier Flagging'\n",
    ")"
   ],
   "id": "33c65c165d85b8ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get inliers",
   "id": "a36301f6d5368640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:50:09.969588500Z",
     "start_time": "2025-11-13T15:06:49.078845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_inliers_df(df: pd.DataFrame, x_col: str, y_col: str, iqr_multiplier: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects and returns the 'inliers' DataFrame by excluding rows that are\n",
    "    outliers in EITHER the x_col or y_col based on the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    x_col : str\n",
    "        The primary numerical column for X-axis comparison.\n",
    "    y_col : str\n",
    "        The primary numerical column for Y-axis comparison.\n",
    "    iqr_multiplier : float\n",
    "        The factor used to define the outlier boundary (default is 1.5).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame containing only the inliers.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Drop NaNs in the relevant columns\n",
    "    data_filtered = df.dropna(subset=[x_col, y_col]).copy()\n",
    "\n",
    "    # 2. Calculate IQR boundaries for X-axis\n",
    "    Q1_x = data_filtered[x_col].quantile(0.25)\n",
    "    Q3_x = data_filtered[x_col].quantile(0.75)\n",
    "    IQR_x = Q3_x - Q1_x\n",
    "    lower_x = Q1_x - iqr_multiplier * IQR_x\n",
    "    upper_x = Q3_x + iqr_multiplier * IQR_x\n",
    "\n",
    "    # 3. Calculate IQR boundaries for Y-axis\n",
    "    Q1_y = data_filtered[y_col].quantile(0.25)\n",
    "    Q3_y = data_filtered[y_col].quantile(0.75)\n",
    "    IQR_y = Q3_y - Q1_y\n",
    "    lower_y = Q1_y - iqr_multiplier * IQR_y\n",
    "    upper_y = Q3_y + iqr_multiplier * IQR_y\n",
    "\n",
    "    # 4. Define the Outlier Mask (True if outside bounds in either dimension)\n",
    "    is_outlier_x = (data_filtered[x_col] < lower_x) | (data_filtered[x_col] > upper_x)\n",
    "    is_outlier_y = (data_filtered[y_col] < lower_y) | (data_filtered[y_col] > upper_y)\n",
    "\n",
    "    # A row is an outlier if it's extreme in X OR extreme in Y\n",
    "    is_outlier = is_outlier_x | is_outlier_y\n",
    "\n",
    "    # 5. Select Inliers (where the mask is False)\n",
    "    inliers_df = data_filtered[~is_outlier].copy()\n",
    "\n",
    "    num_outliers = is_outlier.sum()\n",
    "    print(f\"Total rows removed as bivariate outliers: {num_outliers}\")\n",
    "    print(f\"Total rows remaining (inliers): {len(inliers_df)}\")\n",
    "\n",
    "    return inliers_df"
   ],
   "id": "d83be39a4914580d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaned df that contains only inliers",
   "id": "fbe7218dd0c87563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:50:09.985215100Z",
     "start_time": "2025-11-13T15:06:49.107530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleaned_df = get_inliers_df(\n",
    "    log_transformed_price,\n",
    "    x_col='total_area_sqm',\n",
    "    y_col='log_price'\n",
    ")"
   ],
   "id": "1144e9e5a610baf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows removed as bivariate outliers: 5287\n",
      "Total rows remaining (inliers): 62028\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
